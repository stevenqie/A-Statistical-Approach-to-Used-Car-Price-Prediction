---
title: "A Statistical Approach to Used Car Price Prediction"
output: pdf_document
date: 12/09/2024
header-includes:
  - \usepackage{multicol}
  - \usepackage{fancyvrb}
  - \setlength{\columnsep}{1cm}
  - \usepackage{geometry}
  - \geometry{margin=1in}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\begin{multicols}{3}
\setlength{\columnsep}{2pt}

\begin{center}
\textbf{Steven Qie} \\
\textit{Statistics and Computer Science} \\
\textit{University of Illinois Urbana-Champaign} \\
\textbf{qie2@illinois.edu}
\end{center}

\columnbreak

\begin{center}
\textbf{Brian Gong} \\
\textit{Statistics and Computer Science} \\
\textit{University of Illinois Urbana-Champaign} 
\textbf{brianhg2@illinois.edu}
\end{center}

\columnbreak

\begin{center}
\textbf{William Yeh} \\
\textit{Statistics and Computer Science} \\
\textit{University of Illinois Urbana-Champaign} 
\textbf{wy16@illinois.edu}
\end{center}

\end{multicols}

## Introduction

With the used car market being significantly larger than the new car market, many consumers are realizing that used cars provide a more affordable option. It plays a significant role in the growth and stability of the U.S. economy, driven by changing consumer preferences, economic factors, and the availability of certain cars. Accurately predicting the price of a used car is a challenging but essential task for buyers, sellers, and market analysts/economists alike. 

This report aims to develop various predictive models for used car prices using the Used Car Price Prediction Dataset from Kaggle. This dataset comprises of 4,009 data points, representing unique vehicle listings, as well as nine distinct features that serve as key indicators influencing the value of a used car. We follow a very structured and standard approach, including data exploration, preprocessing, model training, and evaluation using relevant performance metrics. By leveraging these methods, we aim to uncover valuable insights into the world of automobiles and the various factors that are driving used car prices.

Need a section on key findings....



Abstractâ€” 

white space 

white space 

white space 

white space 

white space 

white space 

white space 

white space 

white space 

We utilized AI tools in this report to enhance and assist in our writing. These tools helped play a big role in ensuring clarity, conciseness, and professionalism. We also utilized AI tools to help us with syntax help when writing code in R, as well as discovering potential bugs in our code. 


## Literature Review 
This literature review aims to summarize key findings and approaches from a few noteworthy research papers focused on used car price prediction. 

"Price Prediction of Used Cars Using Machine Learning", written by Chuyang Jin of the University of Sydney, presents a model that can predict a used vehicle's price given their year of production, mileage, tax, miles per gallon, He hopes that his model can benefit and save time for both sellers and buyers who are looking to sell or serach for second-hand vehicles. Jin used a CSV dataset containing 100,000 records of used cars in the UK, focusing specifically on the Mercedes brand. The nine factors that he considered were the following: model, year, selling price, transmission, mileage, fuel type, tax, miles per gallon (mpg), and engine size. While doing exploratory data analysis and preprocessing, Jin noted that many many predictors had skewed distributions. For example, the overwhelming majority of prices fell in the 0-75,000 range, limiting the model's potential effectiveness for higher price ranges. Jin deemed these data points as outliers and excluded them to ensure that the model would be more accurate and usable. After testing various forms of regression, namely linear, polynomial, SVR, Decision Trees, and Random Forests, Jin found Random Forest Regression yielded the best R squared value of 0.90416. 

"Used Car Price Prediction using Machine Learning: A Case Study", written by Mustapha Hankar, Marouane Birjali, and Abderrahim Beni-Hssane, applies several supervised machine learning algorithms to predict used car price prices based on features from a dataset collected from an online eCommerce website called Avito. During preprocessing, the authors of this paper performed recursive feature elimination to maintain only the most relevant features to car prices: year of manufacture, mileage, mark, fuel type, fiscal power, and model. Along with a baseline multiple linear regression model, the study also looked at K-nearest neighbors, Random Forest, Gradient Boosting, and Artificial Neural Networks. The study utilized 2 different performance metrics, R^2 and RMSE, and concluded that the Gradient Boosting Regression Model achieved the best results, with a R^2 of 0.8 and RMSE of 44516.20. 

"Car Price Prediction using Supervised and Unsupervised Learning Models and Deep Learning" by Thomas Nsiah approached the problem of car price prediction from a supervised and unsupervised lenses. While supervised models allow a consumer to understand the key factors and predictors that influence pricing of used cars, unsupervised learning oftentimes uncovers hidden connections and patterns within the data. In his paper, Nsiah used a mock dataset of 50,000 UK second hand car sales with features similar to the previous 2 studies, such as model, engine size, fuel type, year, and mileage. Supervised learning models that Nsiah tried included simple linear regression, polynomial regression, and random forest, evaluated using mean absolute error (MAE) and R-squared metrics. He concluded that out of the supervised models, random forest performed best with an R-squared of 0.99849 and a MAE of 289.0691. For unsupervised learning techniques, Nsiah applied K-Means and DBSCAN clustering to identify price patterns, evaluated using the Davis Boudlin Index and the Silhouette Coefficient. He concluded that K-Means clustering for the year of manufacture vs price produced the best clustering results.

Overall, these three studies demonstrate the effectiveness that machinie learning can have on accurately predicting used car prices. The next section will outline our own approach and findings.

Citations: 

- C. Jin, "Price Prediction of Used Cars Using Machine Learning," in 2021 IEEE International Conference on Emergency Science and Information Technology (ICESIT), Chongqing, China, 2021, pp. 223-230, doi: 10.1109/ICESIT53460.2021.9696839.
- M. Hankar, M. Birjali, and A. Beni-Hssane, "Used Car Price Prediction using Machine Learning: A Case Study," in 2022 11th International Symposium on Signal, Image, Video and Communications (ISIVC), El Jadida, Morocco, 2022, pp. 1-4, doi: 10.1109/ISIVC54825.2022.9800719.
- T. Nsiah, "Car Price Prediction using Supervised and Unsupervised Learning Models and Deep Learning," unpublished, 2024.


## Data Processing and Summary Statistics 

First, we will import the dataset and libraries into our workspace 
```{r}
library(caret)
library(ggplot2)
library(MASS)
library(randomForest)
library(kernlab)
library(stringr)
library(cluster)
library(glmnet)
library(stats)
library(dplyr)
data <- read.csv("used_cars.csv")
```

#Preliminary Data Cleaning/Modifications
First, we will removed the dollar sign and comma in price to enable numeric operations
```{r}
#cat(head(data$price, 5), "\n")
data$price <- as.numeric(gsub("[$,]", "", data$price))
```

The Engine columns contains very useful information such as the horsepower, displacement, cylinders, engine type, and fuel type. We turn these all into new columns. 
```{r}
#head(data$engine, 5)

# Extract Horsepower (HP)
data$horsepower <- as.numeric(str_extract(data$engine, "\\d+\\.\\d+(?=HP)"))

# Extract Displacement
data$displacement <- as.numeric(str_extract(data$engine, "\\d+\\.\\d+(?=L)"))

# Extract Cylinders
data$cylinders <- str_extract(data$engine, "\\d+ Cylinder")

data$cylinders_factor <- factor(str_extract(data$cylinders, "\\d+"))

# Extract Engine Type
data$engine_type <- str_extract(data$engine, "DOHC|SOHC|Turbo|Twin Turbo|Electric Motor")

# Extract Fuel Type
data$fuel_type <- str_extract(data$engine, "Gasoline|Diesel|Electric|Hybrid|Flex Fuel|Plug-In Electric/Gas")

# factor fuel type
data$fuel_type_factor <- factor(data$fuel_type)

# Add <NA> as a valid level in factor types
data$fuel_type_factor <- addNA(data$fuel_type_factor)
data$cylinders_factor <- addNA(data$cylinders_factor)

# Rename the <NA> level to "NA"
levels(data$fuel_type_factor)[is.na(levels(data$fuel_type_factor))] <- "NA"
levels(data$cylinders_factor)[is.na(levels(data$cylinders_factor))] <- "NA"

data$fuel_type_numeric <- as.numeric(data$fuel_type_factor)
data$cylinders_numeric <- as.numeric(data$cylinders_factor)

# numeric value assigned:fuel_type_factor (to get it back)
fuel_type_mapping <- setNames(
  levels(data$fuel_type_factor),
  unique(data$fuel_type_numeric)
)

# numeric value assigned:cylinder_factor
cylinders_mapping <- setNames(
  1:length(levels(data$cylinders_factor)),
  levels(data$cylinders_factor)
)

unique(data$fuel_type_numeric)
fuel_type_mapping
cylinders_mapping
#deleting the engine column because we have extracted all the information 
data$engine = NULL
# colnames(data)
```

Corrected the spelling of mileage from milage to mileage. Removed mi. and , to enable numeric operations
```{r}
colnames(data)[colnames(data) == "milage"] <- "mileage"
data$mileage <- as.numeric(gsub("[,]| mi\\.", "", data$mileage))
```

#Analyzing categorical variables

```{r}
# num model and num brand are too large so will just omit 
length(table(data$brand)) # 57
length(unique(data$model))
length(unique(data$transmission)) # 61

# calculate the counts for brand
brandcounts <- table(data$brand)

barplot(brandcounts,
  main = "Histogram of brand",
  xlab = "brand",
  ylab = "Count",
  col = "skyblue",
  las = 2) 

# calculate the counts for transmission
trancounts <- table(data$transmission)

barplot(trancounts,
  main = "Histogram of transmission",
  xlab = "transmission",
  ylab = "Count",
  col = "skyblue",
  las = 2) 

summary(data)
```

```{r}
result <- data %>%
  group_by(transmission) %>%
  summarise(
    medianprice = median(price),
    averageprice = mean(price),
    count = n()
  )
result_sorted <- result %>%
  arrange(desc(count))
print(result_sorted)
threshold <- quantile(result_sorted$count, 0.9)
threshold
print(colnames(result_sorted))
significant_transmissions <- result_sorted$transmission[result_sorted$count > threshold]
print(significant_transmissions)
sum(is.na(data$transmission))
```

After looking at histograms for both Brand and Transmission, it seems Brand is more uniformly distributed while Transmission has a few salient categories. After exploring the categories of transmissions we found that the top 6 most frequent transmissions account for approximately 67-70% of the data points. Therefore we will one hot encode these 6 categories + an "Other" category for Transmission. 

# One-hot encoding categorical variables 

Transmission, fuel_type should 

cylinders we will just keep as numeric

#Analyzing Null/Empty Values 
We will first look at the problem with NA and Empty values, something that this dataset has a lot of. We will first handle both NA and Empty "" values by replacing them to "NA" to make it easier to preprocess and analyze.  
```{r}
na_columns <- colSums(is.na(data)) > 0
empty_string_columns <- colSums(data == "") > 0
columns_with_na_or_empty <- na_columns | empty_string_columns
print(names(data)[columns_with_na_or_empty])

# data[data == "" | is.na(data)] <- "NA"
# summary(data)
# unique(data$horsepower)
# unique(data$displacement)
# unique(data$cylinders_numeric)
unique(data$fuel_type_factor)
unique(data$fuel_type_numeric)
unique(data$horsepower)
unique(data$displacement)
unique(data$cylinders_factor)
unique(data$cylinders_numeric)
sum(is.na(data$horsepower))
sum(is.na(data$displacement))
table(data$displacement)
summary(data)
```
There are five columns with empty strings/NA values. Let's examine all five of them to discover if we can find any patterns. 

## horsepower 

```{r}
# number of unique values in horsepower
length(table(data$horsepower))

# number of null values in horsepower
sum(is.na(data$horsepower))

# calculate the counts for horsepower
horsepower_counts <- table(data$horsepower)

barplot(horsepower_counts,
  main = "Histogram of Horsepower",
  xlab = "Horsepower",
  ylab = "Count",
  col = "skyblue",
  las = 2) 

# median imputation 
data$horsepower[is.na(data$horsepower)] <- median(data$horsepower, na.rm = TRUE)
```

Since there are 348 unique values in horsepower, we can consider horsepower as a continuous variable rather than categorical. However, there are 810 null values in a dataset with 4009 entries which is over 20% null values. This is too many to simply drop, so we want to perform some form of imputation. Looking at the distribution of horsepowers, we can see that the median is a good representative approximation for the distribution so we will use **median imputation**.

## displacement (engine size)

```{r}
# number of unique values in displacement
# table(data$displacement)
length(table(data$displacement))

# number of null values in displacement
sum(is.na(data$displacement))

# calculate the counts for horsepower
displacement_counts <- table(data$displacement)

barplot(displacement_counts,
  main = "Histogram of Displacement",
  xlab = "Displacement",
  ylab = "Count",
  col = "skyblue",
  las = 2) 

# median imputation 
data$displacement[is.na(data$displacement)] <- median(data$displacement, na.rm = TRUE)
```

There are 61 unique values in displacement (engine size). Although these appear to be discretized measurements (ex: size = 0.8 or size = 3.71 may not make sense), we can treat it as a more continuous predictor for now. There are 396 null values in displacement which is just under 10% null values, so we could consider dropping these. However since the median already exists in the dataset (median = 3.5) we can also proceed with median imputation which is what we did. 

```{r}
result <- data %>%
  group_by(fuel_type) %>%
  summarise(
    medianprice = median(price),
    averageprice = mean(price),
    count = n()
  )
print(result)
unique(data$fuel_type_factor)
```
The NA values for fuel_type have a higher median price and average price than other types, and makes up a significant count of observations so we are going to treat it as a separate category.

```{r}
result <- data %>%
  group_by(cylinders_factor) %>%
  summarise(
    medianprice = median(price),
    averageprice = mean(price),
    count = n()
  )
print(result)
unique(data$cylinders)
```
cylinder 

```{r}
data$accident[data$accident == "NA"] <- "None "
```

```{r}
result <- data %>%
  group_by(accident) %>%
  summarise(
    medianprice = median(price),
    averageprice = mean(price),
    count = n()
  )
print(result)
```
The NA/Empty values for accident exhibit very similar properties to the None reported category, with median price and average price being pretty similar, not to mention a very small percentage of data is represented by this value. Therefore, we replace and combine these observations with the None reported category. Because accident only has 2 unique values now, no accidents and 1 or more accidents, we changed it to 1,0 to be useful for models. 
```{r}
data$accident[data$accident == "NA"] <- "None reported"
#unique(data$accident)
data$accident <- ifelse(data$accident == "At least 1 accident or damage reported", 1, 0)
# unique(data$accident)
```

```{r}
result <- data %>%
  group_by(clean_title) %>%
  summarise(
    medianprice = median(price),
    averageprice = mean(price),
    count = n()
  )
print(result)
```

The NA values for clean_title clearly have a significantly higher median price and will be treated as a separate category. We apply similar reasoning from accident to clean_title. Since there is only "Yes" and NA, we treat all the yes's to 1 and all the NA values to 0.
```{r}
data$clean_title <-ifelse(data$clean_title == "Yes", 1, 0)
unique(data$clean_title)
```

#Removing Outliers 
```{r}
Q1 <- quantile(data$price, 0.25, na.rm = TRUE)
Q3 <- quantile(data$price, 0.75, na.rm = TRUE)
IQR_value <- IQR(data$price, na.rm = TRUE)

# Identify outliers using IQR
lower_bound <- Q1 - 1.5 * IQR_value
upper_bound <- Q3 + 1.5 * IQR_value

outliers <- data[data$price < lower_bound | data$price > upper_bound, ]
print(paste("Number of outliers: ", nrow(outliers), "and average price of these cars: ", round(mean(outliers$price), 2)))
      
#removing these rows from the dataset
data <- data[!(data$price < lower_bound | data$price > upper_bound), ]
```

We remove outliers with 1.5*IQR value. 

#need feature idengtifrication and analysis for categorical and numerical variables 
#One hot encoding for our categorical variables 

LEFTOVER of brians stuff
```{r}
head(data)
na_values <- data[!complete.cases(data), ]
cat("NA Values:", nrow(na_values), "\n")
empty_space_rows <-  data[rowSums(data == "", na.rm = TRUE) > 0, ]
cat("Empty Values:", nrow(empty_space_rows), "\n")
data[data == ""] <- NA
```
Just looking at the data, some notable columns to do some preprocessing is accident,engine. Price and mileage can use some preprocessing to make into a number. Also, milage is spelled wrong. Although there seems to be no NA values, a lot of rows of clean_title are notably empty. Other cols like fuel_type and accident also contain some empty values. We turned empty values into NA values.

```{r}
empty_space_rows <-  data[rowSums(data == "", na.rm = TRUE) > 0, ]
cat("Empty Values:", nrow(empty_space_rows), "\n")
```
END OF LEFTOVER BRIANS STUFF 

#Final Summary Statistics
```{r}
summary(data)
```

## pca?
WILLY STUFF
```{r}
datan <- data[c("model_year", "mileage", "accident", "clean_title", "horsepower", "displacement", "cylinders_factor", "fuel_type_factor", "price")]
summary(datan)
colSums(is.na(datan))
# remove null values but there should be none
data_cleaned <- na.omit(datan) # almost 1000
head(data_cleaned)
dim(data_cleaned)

# remove price response var
clean_no_price <- within(data_cleaned, rm("price"))
head(clean_no_price)
dim(clean_no_price)

# scale 
data_scaled <- scale(clean_no_price)
# pca fit, try princomp or prcomp
pcafit <- princomp(clean_no_price)
pcafit

scores <- pcafit$scores

scores

plot(scores[, 1], scores[, 2], 
     xlab = "Principal Component 1", 
     ylab = "Principal Component 2", 
     main = "PCA: First Two Components",
     col = "blue", pch = 20)

# pca_result <- prcomp(clean_no_price, center = TRUE, scale. = TRUE)

```
END OF WILLY STUFF

## Unsupervised Learning 
Apply at least three clustering algorithms to the processed dataset.
Determine the appropriate number of clusters and discuss the interpretability of these clusters. Do they hold any meaningful distinctions?
Examine whether the clustering results are associated with your outcome variable.

1. KMeans Clustering 
```{r}
data_subset <- data[, c("model_year", "price")]
data_subset <- na.omit(data_subset)
data <- scale(data_subset)
```
We decided to use kmeans to examine the relation between model_year and price, as we noticed a similar examination in one of the papers while doing the literature review

```{r}
set.seed(1)
# Compute silhouette scores for k = 2 to 10
sil_scores <- sapply(2:10, function(k) {
  km <- kmeans(scale(data_subset[, c("model_year", "price")]), centers = k, nstart = 10)
  silhouette(km$cluster, dist(scale(data_subset[, c("model_year", "price")]))) %>%
    summary() %>%
    .$avg.width
})
# Plot silhouette scores
plot(2:10, sil_scores, type = "b", pch = 19, frame = FALSE,
     xlab = "Number of Clusters (k)",
     ylab = "Average Silhouette Score",
     main = "Silhouette Method for Optimal k")

```


```{r}
set.seed(1)  


kmeans_result <- kmeans(data_subset_scaled, centers = 4)

# Add cluster assignments to the original data
data_subset$cluster <- as.factor(kmeans_result$cluster)

# Visualize the clusters
ggplot(data_subset, aes(x = model_year, y = price, color = cluster)) +
  geom_point(size = 2) +
  labs(title = "K-Means Clustering on Model Year vs Price",
       x = "Model Year",
       y = "Price") +
  theme_minimal()


```
Two notable things:
1. There are a couple huge outliers in the price department that is taking up its own cluster
2.The rest of the clusters (2,3,4) are clearly separated by year ranges. There is a slightly higher range/variation in price in the most recent year range, so there may be a weak relationship between a more recent model_year and higher price



2. Hierarchical Clustering 

Next, we will try hierarchical clustering with three different linkage methods(single, complete, and average) using euclidean distance. Hierarchical Clustering begins with each data point starting as its own cluster. The goal is to progressively group them together until there is only one group. The process involves choosing the closest two groups, calculated through a specific distance metric. 
```{r}
set.seed(1)
numeric_data <- data[, sapply(data, is.numeric)]
numeric_data_without_price <- numeric_data[, !colnames(numeric_data) %in% "price"]

hclust_single <- hclust(dist(numeric_data_without_price, method = "euclidean"), method = "single")
hclust_complete <- hclust(dist(numeric_data_without_price, method = "euclidean"), method = "complete")
hclust_average <- hclust(dist(numeric_data_without_price, method = "euclidean"), method = "average")

par(mfrow = c(1, 3))
plot(hclust_single, main = "Single Linkage", xlab = "", sub = "", cex = 0.6)
plot(hclust_complete, main = "Complete Linkage", xlab = "", sub = "", cex = 0.6)
plot(hclust_average, main = "Average Linkage", xlab = "", sub = "", cex = 0.6)
par(mfrow = c(1, 1))

```

```{r}

clusters_complete <- cutree(hclust_complete, k = 4)


data$cluster <- as.factor(clusters_complete)


table(data$cluster)

cluster_summary <- data %>%
  group_by(cluster) %>%
  summarise(
    avg_price = mean(price, na.rm = TRUE),
    avg_model_year = mean(model_year, na.rm = TRUE),
    avg_accident = mean(as.numeric(accident), na.rm = TRUE),  # Replace with proper numeric conversion if needed
    avg_mileage = mean(mileage, na.rm = TRUE),
    avg_horsepower = mean(horsepower, na.rm = TRUE),
    count = n()
  )

# Print the cluster summary
print(cluster_summary)


```


3. Spectral Clustering 

Finally, we will try spectral clustering, which aims to group observations based on their proximity information. This method involves 2 main steps, the first being using the eigenvalues of a similarity matrix to perform dimension reduction, followed by applying a clustering algorithm like K-means. 

```{r}
set.seed(1)
specc_result <- specc(as.matrix(train[, -1]), centers = 3, kernel = "rbfdot")
```


## Prediction Models 
```{r}
head(data)
```
For all the supervised models below, we will split the data into training sets for model training and testing sets to evaluate performance and accuracy
```{r}
y = data$price
X = data.matrix(cbind(1, data[, setdiff(names(data), "price")]))
test_idx = sample(nrow(data), size = 0.2 * nrow(data))
xtrain = X[ -test_idx, ]
xtest  = X[ test_idx, ]
ytrain = y[ -test_idx ]
ytest  = y[ test_idx ]
```

1. Linear Model. There are mainly three possible linear models: Lasso, Ridge, and Elastic Net. We will try all three models and see which one performs the best. Lasso, Ridge, and Elastic Net all benefit from feature scaling because these models involve regularization. which will penalize the size of coefficients of the model to avoid overfitting. All 3 models also involving a tuning parameter, and so we will use k-fold cross validation to find the best parameters. 

```{r}
set.seed(1)
#feature scale the data 
xtraincopy <- xtrain 
xtestcopy <- xtest
xtrain_scaled <- scale(xtraincopy)  
xtest_scaled <- scale(xtestcopy, center = attr(xtrain_scaled, "scaled:center"), scale = attr(xtrain_scaled, "scaled:scale"))
```

Setup for k-fold Cross Validation 
```{r}
train_control <- trainControl(method = "cv", number = 10)  # 10-fold cross-validation

lasso_grid <- expand.grid(alpha = 1, lambda = seq(0.01, 1, by = 0.01))
ridge_grid <- expand.grid(alpha = 0, lambda = seq(0.01, 1, by = 0.01))
elastic_net_grid <- expand.grid(alpha = seq(0, 1, by = 0.1), lambda = seq(0.01, 1, by = 0.01))
```

Training our 3 models 
```{r}
lasso_model <- train(x = xtrain_scaled, y = ytrain, method = "glmnet", trControl = train_control, tuneGrid = lasso_grid)
ridge_model <- train(x = xtrain_scaled, y = ytrain, method = "glmnet", trControl = train_control, tuneGrid = ridge_grid)
elastic_net_model <- train(x = xtrain_scaled, y = ytrain, method = "glmnet", trControl = train_control, tuneGrid = elastic_net_grid)
```

Finding the best tuning parameters for all of them 
```{r}
print(paste("Best tuning parameter for Lasso: ", lasso_model$bestTune))
print(paste("Best tuning parameters for Ridge:", ridge_model$bestTune))
print(paste("Best tuning parameters for Elastic Net:", elastic_net_model$bestTune))
```

Predict on our test set and find prediction error 
```{r}
lasso_pred <- predict(lasso_model, Xtest)
ridge_pred <- predict(ridge_model, Xtest)
elastic_net_pred <- predict(elastic_net_model, Xtest)

print(paste("Lasso model prediction error:", mean((lasso_pred - ytest)^2)))
print(paste("Ridge model prediction error:", mean((ridge_pred - ytest)^2)))
print(paste("Elastic net prediction error: ", mean((elastic_net_pred - ytest)^2)))
```
Out of our 3 linear models, ... performed the best 


2. K Nearest Neighbors(KNN) regression works by calculating the k nearest training set data points to the test point and predicting the target value by taking the average of their target values. KNN is sensitive to feature scaling, so we will need to scale the data. The reason behind this is for example, if one feature has ranges from 1-10 and another one has 1-10000, distance calcualtions will be biased and results will suffer as a result. KNN is also sensitive to the choice of k. To find the optimal value of k, we will perform k-fold cross validation. 

```{r}
set.seed(1)
# feature scale 
xtraincopy <- xtrain 
xtestcopy <- xtest
xtrain_scaled <- scale(xtraincopy)  
xtest_scaled <- scale(xtestcopy, center = attr(xtrain_scaled, "scaled:center"), scale = attr(xtrain_scaled, "scaled:scale"))
```

```{r}
#train knn algorithm and perform k fold cross validation
control <- trainControl(method = "cv", number = 10)
knn.cvfit <- train(y ~ ., method = "knn", 
                     data = data.frame(xtrain_scaled, ytrain,
                     tuneGrid = data.frame(k = seq(1, 40, 1)),
                     trControl = control)
# Plot the cross-validation results to visualize performance
plot(knn_model)

#plot k vs regression error 
plot(knn.cvfit$results$k, 1-knn.cvfit$results$Accuracy,
     xlab = "K", ylab = "Regression Error", type = "b",
     pch = 19, col = "darkorange")

print(paste("The best value of k based on cross-validation is: ", knn_model$bestTune))
```

```{r}
# Train the final model using the best value of k and find the predictions
best_k <- knn_model$bestTune$k
knn_predictions <- knn(train = xtrain_scaled, test = xtest_scaled, cl = ytrain, k = best_k)

# Calculate prediction error 
print(paste("Prediction errort: ", mean((as.numeric(knn_predictions) - ytest)^2)))
```


3. Random Forest 
```{r}
rf_model <- randomForest(train_labels ~ ., data = as.data.frame(train_data))

rf_predictions <- predict(rf_model, newdata = as.data.frame(test_data))
```
4. SVM? does this count as a linear model 
5. Gradient Boosting Regressor 

## Open-Ended Question/Conclusion
A researcher is interested in estimating the original price of the cars in your dataset as if they were brand new. 

Several challenges arise from this approach. 


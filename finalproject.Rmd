---
title: "A Statistical Approach to Used Car Price Prediction"
output:
  pdf_document: default
  word_document: default
date: "12/09/2024"
header-includes:
- \usepackage{multicol}
- \usepackage{fancyvrb}
- \setlength{\columnsep}{1cm}
- \usepackage{geometry}
- \geometry{margin=1in}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.width=7, fig.height=3)

```

\begin{multicols}{3}
\setlength{\columnsep}{2pt}

\begin{center}
\textbf{Steven Qie} \\
\textit{Statistics and Computer Science} \\
\textit{University of Illinois Urbana-Champaign} \\
\textbf{qie2@illinois.edu}
\end{center}

\columnbreak

\begin{center}
\textbf{Brian Gong} \\
\textit{Statistics and Computer Science} \\
\textit{University of Illinois Urbana-Champaign} 
\textbf{brianhg2@illinois.edu}
\end{center}

\columnbreak

\begin{center}
\textbf{William Yeh} \\
\textit{Statistics and Computer Science} \\
\textit{University of Illinois Urbana-Champaign} 
\textbf{wy16@illinois.edu}
\end{center}

\end{multicols}

## Introduction

With the used car market being significantly larger than the new car market, many consumers are realizing that used cars provide a more affordable option. It plays a significant role in the growth and stability of the U.S. economy, driven by changing consumer preferences, economic factors, and the availability of certain cars. Accurately predicting the price of a used car is a challenging but essential task for buyers, sellers, and market analysts/economists alike. 

This report aims to develop various predictive models for used car prices using the Used Car Price Prediction Dataset from Kaggle. This dataset comprises of 4,009 data points, representing unique vehicle listings, as well as nine distinct features that serve as key indicators influencing the value of a used car. We follow a very structured and standard approach, including data exploration, preprocessing, model training, and evaluation using relevant performance metrics. By leveraging these methods, we aim to uncover valuable insights into the world of automobiles and the various factors that are driving used car prices.

Need a section on key findings....



Abstractâ€” 


For unsupervised learning, we simply utilized 3 clustering methods we learned from this course, namely kmeans clustering. hierarchical clustering, and spectral clustering, in order to find any hidden patterns in the dataset. Some parts of the methods that we used, such as the Silhouette Method was inspired by the research papers in the literature review, as we saw it being used there.

white space 

white space 

We utilized AI tools in this report to enhance and assist in our writing. These tools helped play a big role in ensuring clarity, conciseness, and professionalism. We also utilized AI tools to help us with syntax help when writing code in R, as well as discovering potential bugs in our code. 


## Literature Review 
This literature review aims to summarize key findings and approaches from a few noteworthy research papers focused on used car price prediction. 

"Price Prediction of Used Cars Using Machine Learning", written by Chuyang Jin of the University of Sydney, presents a model that can predict a used vehicle's price given their year of production, mileage, tax, miles per gallon, He hopes that his model can benefit and save time for both sellers and buyers who are looking to sell or serach for second-hand vehicles. Jin used a CSV dataset containing 100,000 records of used cars in the UK, focusing specifically on the Mercedes brand. The nine factors that he considered were the following: model, year, selling price, transmission, mileage, fuel type, tax, miles per gallon (mpg), and engine size. While doing exploratory data analysis and preprocessing, Jin noted that many many predictors had skewed distributions. For example, the overwhelming majority of prices fell in the 0-75,000 range, limiting the model's potential effectiveness for higher price ranges. Jin deemed these data points as outliers and excluded them to ensure that the model would be more accurate and usable. After testing various forms of regression, namely linear, polynomial, SVR, Decision Trees, and Random Forests, Jin found Random Forest Regression yielded the best R squared value of 0.90416. 

"Used Car Price Prediction using Machine Learning: A Case Study", written by Mustapha Hankar, Marouane Birjali, and Abderrahim Beni-Hssane, applies several supervised machine learning algorithms to predict used car price prices based on features from a dataset collected from an online eCommerce website called Avito. During preprocessing, the authors of this paper performed recursive feature elimination to maintain only the most relevant features to car prices: year of manufacture, mileage, mark, fuel type, fiscal power, and model. Along with a baseline multiple linear regression model, the study also looked at K-nearest neighbors, Random Forest, Gradient Boosting, and Artificial Neural Networks. The study utilized 2 different performance metrics, R^2 and RMSE, and concluded that the Gradient Boosting Regression Model achieved the best results, with a R^2 of 0.8 and RMSE of 44516.20. 

"Car Price Prediction using Supervised and Unsupervised Learning Models and Deep Learning" by Thomas Nsiah approached the problem of car price prediction from a supervised and unsupervised lenses. While supervised models allow a consumer to understand the key factors and predictors that influence pricing of used cars, unsupervised learning oftentimes uncovers hidden connections and patterns within the data. In his paper, Nsiah used a mock dataset of 50,000 UK second hand car sales with features similar to the previous 2 studies, such as model, engine size, fuel type, year, and mileage. Supervised learning models that Nsiah tried included simple linear regression, polynomial regression, and random forest, evaluated using mean absolute error (MAE) and R-squared metrics. He concluded that out of the supervised models, random forest performed best with an R-squared of 0.99849 and a MAE of 289.0691. For unsupervised learning techniques, Nsiah applied K-Means and DBSCAN clustering to identify price patterns, evaluated using the Davis Boudlin Index and the Silhouette Coefficient. He concluded that K-Means clustering for the year of manufacture vs price produced the best clustering results.

Overall, these three studies demonstrate the effectiveness that machinie learning can have on accurately predicting used car prices. The next section will outline our own approach and findings.

Citations: 

- C. Jin, "Price Prediction of Used Cars Using Machine Learning," in 2021 IEEE International Conference on Emergency Science and Information Technology (ICESIT), Chongqing, China, 2021, pp. 223-230, doi: 10.1109/ICESIT53460.2021.9696839.
- M. Hankar, M. Birjali, and A. Beni-Hssane, "Used Car Price Prediction using Machine Learning: A Case Study," in 2022 11th International Symposium on Signal, Image, Video and Communications (ISIVC), El Jadida, Morocco, 2022, pp. 1-4, doi: 10.1109/ISIVC54825.2022.9800719.
- T. Nsiah, "Car Price Prediction using Supervised and Unsupervised Learning Models and Deep Learning," unpublished, 2024.


## Data Processing and Summary Statistics 


```{r, include=FALSE}
library(caret)
library(ggplot2)
library(MASS)
library(randomForest)
library(kernlab)
library(stringr)
library(cluster)
library(glmnet)
library(stats)
library(dplyr)
library(class)
library(e1071)
library(caret)

# Load necessary libraries

data <- read.csv("used_cars.csv")
```

### Preliminary Data Cleaning/Modifications

Basic Preprocessing:
First we removed the dollar sign and comma in price to enable numeric operations. Then we removed mi. and , to enable numeric operation for milage as well. We also corrected the spelling of mileage from milage to mileage. The Engine column contains very useful information such as the horsepower, displacement, cylinders, engine type, and fuel type all in one column so we made each one into its own separate column. 
```{r, echo=FALSE}
data$price <- as.numeric(gsub("[$,]", "", data$price))
```


```{r, echo=FALSE}
colnames(data)[colnames(data) == "milage"] <- "mileage"
data$mileage <- as.numeric(gsub("[,]| mi\\.", "", data$mileage))
```


```{r, include=FALSE}
# Extract Horsepower (HP)
data$horsepower <- as.numeric(str_extract(data$engine, "\\d+\\.\\d+(?=HP)"))

# Extract Displacement
data$displacement <- as.numeric(str_extract(data$engine, "\\d+\\.\\d+(?=L)"))

# Extract Cylinders
data$cylinders <- str_extract(data$engine, "\\d+ Cylinder")
#data$cylinders_factor <- factor(str_extract(data$cylinders, "\\d+"))

# Extract Engine Type
data$engine_type <- str_extract(data$engine, "DOHC|SOHC|Turbo|Twin Turbo|Electric Motor")

# Extract Fuel Type
data$fuel_type <- str_extract(data$engine, "Gasoline|Diesel|Electric|Hybrid|Flex Fuel|Plug-In Electric/Gas")
#data$fuel_type_factor <- factor(data$fuel_type)

#we are done with engine column since we have extracted all the information out 
data$engine = NULL
head(data)
```

```{r, include=FALSE}
sapply(data, class)
sapply(data, function(col) {
  if (is.character(col)) {
    length(unique(col))
  } else {
    NA  # Return NA for non-character columns
  }
})
```


```{r, include=FALSE}
na_columns <- colSums(is.na(data)) > 0
empty_string_columns <- colSums(data == "") > 0
columns_with_na_or_empty <- na_columns | empty_string_columns
print(names(data)[columns_with_na_or_empty])
```

### Analyzing categorical variables
Categorical variables with various unique values include brand, model, transmission, ext_col, int_col. Let's examine all of them

First, we look at the "brand" and the "model" columns. Through analysis shown below, we have decided to omit both of these columns. Our reasoning and visualizations are shown below. 

There are 57 unique brands with the frequency histogram not showing much dominance in a certain brand. To reduce the dimensionality, we will just omit this column 
```{r, echo=FALSE}
length(unique(data$brand)) 
# calculate the counts for brand
brandcounts <- table(data$brand)
#barplot(brandcounts,
  #main = "Histogram of brand",
  #xlab = "brand",
  #ylab = "Count",
  #col = "skyblue",
  #las = 2) 

result <- data %>%
  group_by(brand) %>%
  summarise(
    medianprice = median(price),
    averageprice = mean(price),
    count = n()
  )
result_sorted <- result %>%
  arrange(desc(count))
print(result_sorted)
#omit this column 
data$brand = NULL
```
A similar problem is seen in the model column. We also omit this column from the dataset 
```{r, include=FALSE}
length(unique(data$model))
modelcounts <- table(data$model)
barplot(modelcounts,
  main = "Histogram of model",
  xlab = "model",
  ylab = "Count",
  col = "skyblue",
  las = 2) 

#omit this columm 
data$model = NULL
```

Now, let's examine colors. There are both intcol and extcol variables. Having too many unique color names can introduce noise into your classification model and make it harder for the model to generalize effectively. Grouping the colors into broader, more general categories can help improve model performance by reducing the dimensionality of the feature and making patterns more apparent.
```{r, include=FALSE}
# Define the mapping function
generalize_colors <- function(color_name) {
  # Convert to lowercase for uniformity
  color_lower <- tolower(color_name)
  
  # Define patterns for each general category
  if (str_detect(color_lower, "black")) {
    return("Black")
  } else if (str_detect(color_lower, "white|ivory|platinum")) {
    return("White")
  } else if (str_detect(color_lower, "gray|grey|silver|slate|charcoal|mica|metallic|graphite")) {
    return("Gray")
  } else if (str_detect(color_lower, "brown|beige|tan|camel|mocha|walnut|chestnut|saddle|cappuccino|cocoa")) {
    return("Brown")
  } else if (str_detect(color_lower, "silver")) {
    return("Silver")
  } else if (str_detect(color_lower, "gold")) {
    return("Gold")
  } else {
    return("Other")  # For colors that don't match any category
  }
}
```

```{r, include=FALSE}
length(unique(data$ext_col))
extcolorcounts <- table(data$ext_col)
barplot(extcolorcounts,
  main = "Histogram of ext_col",
  xlab = "model",
  ylab = "Count",
  col = "skyblue",
  las = 2) 
```
We narrowed down the colors to 6 generalized colors
```{r, echo=FALSE}
data$ext_col <- sapply(data$ext_col, generalize_colors)
unique(data$ext_col)
```


The same thing happens to int_col, but looking at the dataset we decided to have 4 categories. 
```{r, echo=FALSE}
#length(unique(data$int_col))
intcolorcounts <- table(data$int_col)
#barplot(intcolorcounts,
  #main = "Histogram of interior color",
  #xlab = "model",
  #ylab = "Count",
  #col = "skyblue",
  #las = 2) 

# Grouping less frequent categories
data$int_col <- ifelse(
  data$int_col %in% c("Black", "Jet Black", "AMG Black"),
  "Black",
  ifelse(data$int_col %in% c("Beige", "Ivory"), "Beige/Ivory",
         ifelse(data$int_col %in% c("Gray", "Graphite"), "Gray",
                "Other"))
)

unique(data$int_col)
```


```{r, include=FALSE}
length(unique(data$transmission))
# calculate the counts for transmission
trancounts <- table(data$transmission)

barplot(trancounts,
  main = "Histogram of transmission",
  xlab = "transmission",
  ylab = "Count",
  col = "skyblue",
  las = 2) 

result <- data %>%
  group_by(transmission) %>%
  summarise(
    medianprice = median(price),
    averageprice = mean(price),
    count = n()
  )
result_sorted <- result %>%
  arrange(desc(count))
print(result_sorted)
threshold <- quantile(result_sorted$count, 0.9) # 205.7
significant_transmissions <- result_sorted$transmission[result_sorted$count > threshold]
print(significant_transmissions)
```



```{r, include=FALSE}
# map transmissions to just the top 7 or Other. could make this function take in significant_transmissions too for extensibility
map_transmission <- function(transmission) {
  primary_transmissions <- c(
    "A/T", 
    "8-Speed A/T", 
    "Transmission w/Dual Shift Mode", 
    "6-Speed A/T", 
    "6-Speed M/T", 
    "Automatic", 
    "7-Speed A/T"
  )
  
  if (transmission %in% primary_transmissions) {
    return(transmission)
  } else {
    return("Other")
  }
}

# Apply the mapping function
data$transmission <- sapply(data$transmission, map_transmission)
```

### Analyzing Null/Empty Values 
We will first look at the problem with NA and Empty values, something that this dataset has a lot of. We will first handle both NA and Empty "" values by replacing them to "NA" to make it easier to preprocess and analyze.  
```{r, include=FALSE}
na_columns <- colSums(is.na(data)) > 0
empty_string_columns <- colSums(data == "") > 0
columns_with_na_or_empty <- na_columns | empty_string_columns
print(names(data)[columns_with_na_or_empty])
```
There are several features with empty strings/NA values. Let's examine all them to discover if we can find any patterns. 

### horsepower 

```{r, include=FALSE}
# number of unique values in horsepower
length(table(data$horsepower))

# number of null values in horsepower
sum(is.na(data$horsepower))

# calculate the counts for horsepower
horsepower_counts <- table(data$horsepower)

barplot(horsepower_counts,
  main = "Histogram of Horsepower",
  xlab = "Horsepower",
  ylab = "Count",
  col = "skyblue",
  las = 2) 

# median imputation 
data$horsepower[is.na(data$horsepower)] <- median(data$horsepower, na.rm = TRUE)

sum(is.na(data$horsepower))
```

Since there are 348 unique values in horsepower, we can consider horsepower as a continuous variable rather than categorical. However, there are 810 null values in a dataset with 4009 entries which is over 20% null values. This is too many to simply drop, so we want to perform some form of imputation. Looking at the distribution of horsepowers, we can see that the median is a good representative approximation for the distribution so we will use **median imputation**.

### displacement (engine size)

```{r, include=FALSE}
# number of unique values in displacement
# table(data$displacement)
length(table(data$displacement))

# number of null values in displacement
sum(is.na(data$displacement))

# calculate the counts for horsepower
displacement_counts <- table(data$displacement)

barplot(displacement_counts,
  main = "Histogram of Displacement",
  xlab = "Displacement",
  ylab = "Count",
  col = "skyblue",
  las = 2) 

# median imputation 
data$displacement[is.na(data$displacement)] <- median(data$displacement, na.rm = TRUE)

sum(is.na(data$displacement))
```

There are 61 unique values in displacement (engine size). Although these appear to be discretized measurements (ex: size = 0.8 or size = 3.71 may not make sense), we can treat it as a more continuous predictor for now. There are 396 null values in displacement which is just under 10% null values, so we could consider dropping these. However since the median already exists in the dataset (median = 3.5) we can also proceed with median imputation which is what we did. 

```{r, include=FALSE}
result <- data %>%
  group_by(fuel_type) %>%
  summarise(
    medianprice = median(price),
    averageprice = mean(price),
    count = n()
  )
print(result)
data$fuel_type[is.na(data$fuel_type)] <- "NA"

```
The NA values for fuel_type have a higher median price and average price than other types, and makes up a significant count of observations so we are going to treat it as a separate category.


```{r, include=FALSE}
result <- data %>%
  group_by(cylinders) %>%
  summarise(
    medianprice = median(price),
    averageprice = mean(price),
    count = n()
  )
print(result)
data$cylinders[is.na(data$cylinders)] <- "NA"
```

### accident 
```{r, include=FALSE}
result <- data %>%
  group_by(accident) %>%
  summarise(
    medianprice = median(price),
    averageprice = mean(price),
    count = n()
  )
print(result)
```
The NA/Empty values for accident exhibit very similar properties to the None reported category, with median price and average price being pretty similar, not to mention a very small percentage of data is represented by this value. Therefore, we replace and combine these observations with the None reported category. Because accident only has 2 unique values now, no accidents and 1 or more accidents, we changed it to 1,0 to be useful for models. 
```{r, echo=FALSE}
data$accident[data$accident == "NA"] <- "None reported"
#unique(data$accident)
data$accident <- ifelse(data$accident == "At least 1 accident or damage reported", 1, 0)
# unique(data$accident)
```

### clean_title

```{r, include=FALSE}
result <- data %>%
  group_by(clean_title) %>%
  summarise(
    medianprice = median(price),
    averageprice = mean(price),
    count = n()
  )
print(result)
```

The NA values for clean_title clearly have a significantly higher median price and will be treated as a separate category. We apply similar reasoning from accident to clean_title. Since there is only "Yes" and NA, we treat all the yes's to 1 and all the NA values to 0.
```{r, include=FALSE}
data$clean_title <-ifelse(data$clean_title == "Yes", 1, 0)
unique(data$clean_title)
```


```{r, include=FALSE}
# calculate the counts for horsepower
engine_counts <- table(data$engine_type)

barplot(engine_counts,
  main = "Histogram of engine_type",
  xlab = "Engine Type",
  ylab = "Count",
  col = "skyblue",
  las = 2) 

result <- data %>%
  group_by(engine_type) %>%
  summarise(
    medianprice = median(price),
    averageprice = mean(price),
    count = n()
  )
print(result)

data$engine_type[is.na(data$engine_type)] <- "NA"
```

### fuel type/engine type/cylinder
We decided to make NA it's own category for these categorical variables by factoring the features. This is because after analyzing the relationship between price and every level of each categorical variable we found that NA had its own median price that is distinct from the other levels, so we couldn't set the NA values to a default level.



### Removing Outliers 
We remove outliers with 1.5*IQR value.
```{r, echo=FALSE}
Q1 <- quantile(data$price, 0.25, na.rm = TRUE)
Q3 <- quantile(data$price, 0.75, na.rm = TRUE)
IQR_value <- IQR(data$price, na.rm = TRUE)

# Identify outliers using IQR
lower_bound <- Q1 - 1.5 * IQR_value
upper_bound <- Q3 + 1.5 * IQR_value

outliers <- data[data$price < lower_bound | data$price > upper_bound, ]
print(paste("Number of outliers: ", nrow(outliers), "and average price of these cars: ", round(mean(outliers$price), 2)))
      
#removing these rows from the dataset
data <- data[!(data$price < lower_bound | data$price > upper_bound), ]
```


```{r, include=FALSE}
data[sapply(data, is.character)] <- lapply(data[sapply(data, is.character)], as.factor)
```


### one hot encoding
We will now one-hot encode the categorical variables: 
After looking at histograms for both Brand and Transmission, it seems Brand is more uniformly distributed while Transmission has a few salient categories. After exploring the categories of transmissions we found that the top 7 most frequent transmissions account for approximately 67-70% of the data points. Therefore we will one hot encode these 7 categories + an "Other" category for Transmission for a total of 8 transmission categories. We will also one hot encode "fuel type" and "cylinders" since those are categorical variables as well.
```{r, include=FALSE}
dummy_model <- dummyVars(~ ., data = data)
data_one_hot <- as.data.frame(predict(dummy_model, newdata = data))
```


### Final Summary Statistics
```{r, echo=FALSE}
dim(data_one_hot)
summary(data_one_hot)
```

## Unsupervised Learning 


1. KMeans Clustering 
```{r, echo=FALSE}
data_subset <- data[, c("model_year", "price")]
data_subset <- na.omit(data_subset)
data_subset_scaled <- scale(data_subset)
```
We decided to use kmeans to examine the relation between model_year and price, as we noticed a similar examination in one of the papers while doing the literature review. Because K-means utilizes distance metrics, we scale the data before clustering.

```{r, echo=FALSE}
set.seed(1)

sil_scores <- sapply(2:10, function(k) {
  km <- kmeans(scale(data_subset_scaled[, c("model_year", "price")]), centers = k, nstart = 10)
  silhouette(km$cluster, dist(scale(data_subset[, c("model_year", "price")]))) %>%
    summary() %>%
    .$avg.width
})

plot(2:10, sil_scores, type = "b", pch = 19, frame = FALSE,
     xlab = "Number of Clusters (k)",
     ylab = "Average Silhouette Score",
     main = "Silhouette Method for Optimal k")

```
We decided to use the Silhouette Method to determine the optimal number of clusters. This method essentially uses distance measures calculating how close clusters are to themselves and how far away they are to other clusters to judge the optimal number of clusters. In this case, 2 has the highest average silhouette score so we will use k=2.


```{r, echo=FALSE}
set.seed(1)  

kmeans_result <- kmeans(data_subset_scaled, centers = 2)
data_subset$cluster <- as.factor(kmeans_result$cluster)

ggplot(data_subset, aes(x = model_year, y = price, color = cluster)) +
  geom_point(size = 2) +
  labs(title = "K-Means Clustering on Model Year vs Price",
       x = "Model Year",
       y = "Price") +
  theme_minimal()

```
There seems to be a pretty solid relationship between a more recent model_year and higher price. Although the 2 clusters seem to be mostly dominated by model year, it's clear that the average price of cluster 2 is higher than cluster 1.



2. Hierarchical Clustering 

Next, we will try hierarchical clustering with three different linkage methods(single, complete, and average) using euclidean distance. Hierarchical Clustering begins with each data point starting as its own cluster. The goal is to progressively group them together until there is only one group. The process involves choosing the closest two groups, calculated through a specific distance metric. 
```{r, echo=FALSE}
#numeric_data <- data[, c("model_year", "price")]
numeric_data <- data[, sapply(data, is.numeric)]
numeric_data_without_price <- numeric_data[, !colnames(numeric_data) %in% "price"]

```
Removing non-numeric features as clustering requires numeric features. Also, removed the target feature price.

```{r, echo=FALSE}

# Perform hierarchical clustering with scaled data
hclust_single <- hclust(dist(numeric_data_without_price, method = "euclidean"), method = "single")
hclust_complete <- hclust(dist(numeric_data_without_price, method = "euclidean"), method = "complete")
hclust_average <- hclust(dist(numeric_data_without_price, method = "euclidean"), method = "average")

# Plot the dendrograms
par(mfrow = c(1, 3))  # Arrange plots side by side
plot(hclust_single, main = "Single Linkage", xlab = "", sub = "", cex = 0.6)
plot(hclust_complete, main = "Complete Linkage", xlab = "", sub = "", cex = 0.6)
plot(hclust_average, main = "Average Linkage", xlab = "", sub = "", cex = 0.6)
par(mfrow = c(1, 1))  # Reset plotting layout


```



```{r, echo=FALSE}
# Cut the dendrogram into 2 clusters using complete linkage
clusters_complete <- cutree(hclust_complete, k = 2)

# Assign cluster labels to the dataset
data$cluster <- as.factor(clusters_complete)

# View the cluster sizes
table(data$cluster)

# Summarise cluster statistics
library(dplyr)

cluster_summary <- data %>%
  group_by(cluster) %>%
  summarise(
    avg_price = mean(price, na.rm = TRUE),
    avg_model_year = mean(model_year, na.rm = TRUE),
    avg_accident = mean(as.numeric(accident), na.rm = TRUE),  # Convert accident to numeric if necessary
    avg_mileage = mean(mileage, na.rm = TRUE),
    avg_horsepower = mean(horsepower, na.rm = TRUE),
    count = n()
  )

# Print the cluster summary
print(cluster_summary)




```
There are a lot of correlations here that make sense between the 2 clusters. Cluster 1, with a more recent avg_model_year, also has a lower avg_mileage and a lower avg_accident rate, probably because the car has been driven for less time, this cluster also has a much higher avg_price in comparison to cluster 2. The data isn't distributed very well however as a vast majority of the points sit in cluster 1, perhaps suggesting that hierarchical clustering isn't suitable for this dataset.


3. Spectral Clustering 

Finally, we will try spectral clustering, which aims to group observations based on their proximity information. This method involves 2 main steps, the first being using the eigenvalues of a similarity matrix to perform dimension reduction, followed by applying a clustering algorithm like K-means. 

```{r, echo=FALSE}

# Step 1: Prepare and scale data
# Select numeric columns only
numeric_data <- data[, sapply(data, is.numeric)]  
numeric_data <- numeric_data[sample(nrow(numeric_data)), ]
numeric_data_without_price <- numeric_data[, !colnames(numeric_data) %in% "price"]  # Exclude 'price' column
numeric_data_without_price_scaled <- scale(numeric_data_without_price)  # Scale the data

# Step 2: Subset the first 1000 points
subset_data <- numeric_data_without_price_scaled[1:1000, ]

# Step 3: Perform spectral clustering
set.seed(1)  # For reproducibility
n_clusters <- 2  # Number of clusters
specc_result <- specc(as.matrix(subset_data), centers = n_clusters, kernel = "rbfdot")


data$cluster <- NA  # Initialize cluster column
data$cluster[1:1000] <- as.factor(specc_result@.Data)  # Assign clusters to the first 1000 points

# Step 5: Summarize the cluster
cluster_summary <- data %>%
  filter(!is.na(cluster)) %>%
  group_by(cluster) %>%
  summarise(
    avg_model_year = mean(model_year, na.rm = TRUE),
    avg_mileage = mean(mileage, na.rm = TRUE),
    avg_accident = mean(as.numeric(accident), na.rm = TRUE),  
    avg_horsepower = mean(horsepower, na.rm = TRUE),
    count = n()
  )


print(cluster_summary)


ggplot(data %>% filter(!is.na(cluster)), aes(x = model_year, y = mileage, color = cluster)) +
  geom_point(size = 2) +
  labs(title = "Spectral Clustering Results", x = "Model Year", y = "Mileage") +
  theme_minimal()




```
We ran Spectral Clustering on a randomly selected 1000 row subset for the data as the computation time was taking too long for the full dataset.

Similar to Cluster 1, with a more recent avg_model_year, also has a lower avg_mileage and a lower avg_accident rate, this cluster also has a much higher avg_price in comparison to cluster 2. The distribution of data points between the 2 clusters seem to be more even in comparison to hierarchically clustering, meaning that perhaps spectral clustering is more suitable for this dataset.


## Prediction Models 

For our supervised models, we will divide the data into training and testing sets using an 80/20 split. The training set (80% of the data) will be used to train the models, while the testing set (20% of the data) will be reserved to evaluate their performance and accuracy.
```{r, echo=FALSE}
y <- data_one_hot$price
X <- data_one_hot[, !(colnames(data_one_hot) %in% "price")]
sample <- sample(c(TRUE, FALSE), nrow(data), replace=TRUE, prob=c(0.8, 0.2))
xtrain <- X[sample, ]
xtest <- X[!sample, ]
ytrain = y[sample]
ytest = y[!sample] 
```

### 1. Linear Model(Lasso, Ridge, Elastic Net)

Lasso, Ridge, and Elastic net are techniques used in linear regression to improve generalization and prevent overfitting. These regularization techniques work by penalizing the size of coefficients of the model and work well when dealing with high-dimensional data. All 3 models also involve tuning parameters, which control the strength of the penalty, so k-fold cross validation will be used to find the optimal parameters. The cv.glmnet function trains these models and also automatically scales and centers the data. 
```{r}
#Training the models
ridgemodel = cv.glmnet(x = as.matrix(xtrain), y = ytrain, nfolds = 10, alpha = 0)
lassomodel = cv.glmnet(x = as.matrix(xtrain), y = ytrain, nfolds = 10, alpha = 1)
elastic_net_model <- cv.glmnet(x = as.matrix(xtrain), y = ytrain, nfolds = 10, alpha = 0.5)
```

To assess and compare performance of these 3 models, we utilize RMSE, or Root Mean Squared Error. RMSE measures the average difference between the values predicted by a model and the actual values. A lower RMSE indicates better model performance, as it means there are smaller differences between the predicted and actual values. 
```{r, results = "hide"}
ridgepred = predict(ridgemodel, newx = as.matrix(xtest), s = "lambda.min")
lassopred = predict(lassomodel, newx = as.matrix(xtest), s = "lambda.min")
elasticpred = predict(elastic_net_model, newx = as.matrix(xtest), s = "lambda.min")

#RMSE Calculations
ridge_rmse = sqrt(mean((ridgepred - ytest)^2)) #outputs 12017.06

lasso_rmse = sqrt(mean((lassopred - ytest)^2)) #outputs 11992.87

elasticnet_rmse = sqrt(mean((elasticpred - ytest)^2)) #outpus 11993.98
```

```{r, echo = FALSE }
best_lambda <- lassomodel$lambda.min
lasso_coefficients <- coef(lassomodel, s = best_lambda)
```
Out of the three models, the Lasso model performs the best, with a RMSE of 11992.87. Lasso is different from Ridge in that it can shrink coefficients to exactly 0, making it useful for cases when the data has useless features. Examining the coefficients of Lasso, it seems that the most predictive variables include model_year, fuel_type.Diesel, displacement, engine_type.TwinTurbo, and mileage. This seems to suggest that variables related to a car's power and performance are key in determining a price of a used car. 


### 2. K Nearest Neighbors(KNN) 

KNN regression works by calculating the k nearest training set data points to the test point and predicting the target value by taking the average of their target values. Because KNN is a distance based algorithm, it is sensitive to feature scaling. For example, if one feature has ranges from 1-10 and another one has 1-10000, distance calculations will be biased and results will suffer as a result. In addition to standardizing and normalizing our data, we will also perform Principal Component Analysis(PCA) to further reduce the dimensionality of our data and reduce noise, while retaining most of the overall variance. We will keep components that make up 90% of the explained variance. Finally, KNN is also sensitive to the choice of k. To find the optimal value of k, we will again perform k-fold cross validation. We will again, use RMSE to evaluate the performance of our model. 
```{r, include=FALSE}
set.seed(1)

# 2. Scale the numeric columns
preProcValues <- preProcess(xtrain, method = c("center", "scale"))

# Apply scaling and centering to the training and test data
xtrain_processed <- predict(preProcValues, xtrain)
xtest_processed <- predict(preProcValues, xtest)

pca_model <- prcomp(xtrain_processed, center = TRUE, scale. = TRUE)
explained_variance <- summary(pca_model)$importance[3, ]  # Cumulative variance
num_components <- which(explained_variance >= 0.85)[1]   # First component to reach 90%

# Transform the training and testing data
xtrain_pca <- pca_model$x[, 1:num_components]  # Retain the first num_components
xtest_pca <- as.matrix(xtest_processed) %*% pca_model$rotation[, 1:num_components]
```

```{r, echo = FALSE}
# 3. Train the KNN model using the processed data
control <- trainControl(method = "cv", number = 10)
knn.cvfit <- train(ytrain ~ ., method = "knn", 
                   data = data.frame(xtrain_pca, ytrain),
                   tuneGrid = data.frame(k = seq(1, 20, 1)),
                   trControl = control)

# 4. Plot the cross-validation results
plot(knn.cvfit)
```
This graph not only illustrates the cross-validation results for KNN, but also highlights a fundamental concept in machine learning: The Bias-Varaince Trade Off. At low values of K, the RMSE is high due to overfitting, which corresponds to high variance and low bias. The model performs well on the training data, as it relies on very few neighbors, but becomes sensitive when exposed to new testing data. In contrast, high k values correspond to low variance and high bias. The model is more generalized, but it may start to underfit, reflected in the slow increase of RMSE as the k becomes larger and larger. The optimal K achieved through cross validation, K = 5, balances bias and variance to achieve the lowest RMSE. 

Finally, we train a knn model using this optimal k and calculate the RMSE, which is found to be 39728.95
```{r}
best_k <- knn.cvfit$bestTune$k #outputs 5
knn_predictions <- knn(xtrain_processed, xtest_processed, ytrain, best_k)

#RMSE Calculation
knn_rmse = sqrt(mean((as.numeric(knn_predictions) - ytest)^2)) #outputs 39728.95
```

### 3. SVM
Support Vector Machines(SVM) are another supervised machine learning model that excels in high-dimensional spaces, as well as in memory due to the use of support vectors. Their ability to utilize various different kernel functions, both linear and non-linear, allow them to handle complex decision boundaries and capture nonlinear relationships in the data effectively. The radial kernel is a popular kernel choice, and involves 2 key tuning parameters: C and Sigma. Multiple SVM models were fit using different values of C and Sigma, and K-Fold Cross Validation was used to find the combination with the lowest RMSE. 
```{r}
train_control <- trainControl(method = "cv", number = 5)  
tune_grid <- expand.grid(
  C = c(0.1, 1, 5, 7, 10),         #different values of C
  sigma = c(0.01, 0.1, 0.5, 0.7, 1)    # different values of sigma 
)
```

The Best Parameters for C and Sigma were found to be C=7 and Sigma=0.01. The RMSE for this combination was found to be 11054.76
```{r}
svm_model <- train(
  x = as.matrix(xtrain),
  y = as.vector(ytrain),
  method = "svmRadial",       # using the radial kernel
  trControl = train_control,  
  tuneGrid = tune_grid      
)
optimal_parameters <- svm_model$bestTune #outputs C = 7 and sigma = 0.01
svm_predictions <- predict(svm_model, newdata = as.matrix(xtest))

#RMSE Calculation
svm_rmse <- sqrt(mean((svm_predictions - ytest)^2)) #outputs 11054.76
```

### 5. Random Forest 
Random Forest is another powerful supervised machine learning algorithm that involves building numerous decision trees using different subsets of the data, acquired through a technique called bootstrapping. Again, we use RMSE to evaluate the performance since that metric is directly comparable with other models. Parameter tuning was done with standard grid search over the 3 parameters of ntree, mtry, and nodesize. 

- ntree represents the number of decision trees in our model 
- mtry represents the number of features randomly selected at each node split
- nodesize represents the minimum number of observations in a leaf node. 
Each parameter influences the model's bias-variance tradeoff, so careful experimentation is required to find their optimal values. 
```{r echo = FALSE}
set.seed(1)
# redefine with factored versions directly, don't need one hot
y_factor = data$price
X_factor <- data[, !(colnames(data) %in% "price") & !(colnames(data) %in% "cluster")]
# using xtrain, ytrain
train_index <- createDataPartition(y_factor, p = 0.8, list = FALSE)
X_train <- X_factor[train_index, ]
y_train <- y_factor[train_index]
X_test <- X_factor[-train_index, ]
y_test <- y_factor[-train_index]
```

```{r eval = FALSE}
tune_results <- expand.grid(
  ntree = c(100, 200, 300, 400, 500),
  mtry = c(5, ncol(X_train)),
  nodesize = c(1, 5, 30),
  sampsize = nrow(X_train) # maximum
)
```

```{r echo = FALSE }
tune_results <- expand.grid(
  ntree = c(500),
  mtry = c(5),
  nodesize = c(1),
  sampsize = nrow(X_train) # maximum
)
```

```{r}
# tune_results
best_model <- NULL
best_params <- NULL #ends up evaluating to 500, 5, 1
lowest_mse <- Inf 
for (i in 1:nrow(tune_results)) {
  params <- tune_results[i, ]
  model <- randomForest(X_train, 
                        y_train, 
                        ntree = params$ntree,
                        mtry = params$mtry,
                        nodesize = params$nodesize,
                        sampsize = params$sampsize)
  y_pred <- predict(model, X_test)
  mse <- mean((y_pred - y_test)^2)
  if (mse < lowest_mse) {
    lowest_mse <- mse
    best_model <- model
    best_params <- c(params$ntree, params$mtry, params$nodesize)
  }
}
randomforest_rmse = sqrt(lowest_mse) #evalulates to 9497.301
```
Our final Random Forest model has ntree = 500, mtry = 5, and nodesize = 1. This model achieves an RMSE of 9497.301, the lowest among all models we've tested, aligning perfectly with the findings of one of the literature studies we looked at. 

A Random Forest model also provides an advantage with its interpretability. It's able to provide the importance of variables, as seen in the plot below. From the plot, we can see the most influential variables in descending order are as follows: mileage, horsepower, model_year, and displacement. These results also some overlap in the importance of variables between our linear model and regression trees.
```{r}
varImpPlot(best_model)
```
 

## Open-Ended Question/Conclusion
A researcher is interested in estimating the original price of the cars in our dataset as if they were brand new. To solve this problem, we built a machine learning model using features that we believed are most relevant to understanding depreciation. We utilized the following key features to model depreciation: 

1. Brand: Different brands might exhibit varying depreciation patterns. For example, economy brands like Toyota may have a more linear depreciation curve, while luxury brands like BMW may see a steeper initial depreciation. Whether a brand is considered luxury or economy is useful as well, because luxury brands typically have higher starting prices than economy brands. Because of these reasons, we decided to include this feature even though it was omitted in our earlier prediction models. 
2. Age: Cars will generally lose value over time as newer models with newer designs and updated features are released. Since we are only given the model_year in the dataset, which represents when the car was made, we created a new column called "Age" that is simply current_year - model_year + 1, which represents the number of years since the car was new. 
3. Mileage: Generally, the higher the mileage, the lower the car's value. Increased miles often indicates more wear and tear on car components. High mileage on a car can also signal that more maintenance will be required to keep the car running smoothly, making it less attractive to potential buyers. 
4. Accident History: This feature helps capture the impact of damage on a car's value because accidents are usually correlated with the integrity, safety, and reliability of the car. Many buyers will perceive cars with accident histories as less favorable compared to cars without accident histories. 
5. Clean Title: Cars without a clean title may depreciate faster due to the higher perceived risks and uncertaintites assocaited with its condition. 

For practicality reasons, and to manage potential model complexity issues that arises from having to one-hot encoding every single unique brand, we will train our model on a subset of the data, specifically cars belonging to the seven most common brands in our dataset: Ford, BMW, Mercedes-Benz, Chevrolet, Porsche, Audi, and Toyota. This subset also provides a good balance between luxary and economy brands. 

Estimating brand-new car prices using only a dataset of used cars presents a couple of challenges. Because the model has never actually seen cars with zero age and zero mileage, it must infer/extrapoalte what the original prices might have been, which can lead to potential inaccuracies. Additionally, external factors that the data set doesnâ€™t capture, such as inflation, shortages, and market competition, also influence the car's original price. For example, the first six weeks of 2022 saw a car shortage caused by factors such as the pandemic and a global semiconductor shortage, none of which are accounted for in the model. 

Nevertheless, we aim to leverage these features to develop a machine learning model that can reasonably estimate the original prices of cars.

```{r, echo=FALSE}
data <- read.csv("used_cars.csv")
data <- data %>%
  select(brand, model_year, milage, accident, clean_title, price)
data <- data %>% filter(brand %in% c("Ford", "BMW", "Mercedes-Benz", "Chevrolet", "Porsche", "Audi", "Toyota"))

data$price <- as.numeric(gsub("[$,]", "", data$price))
colnames(data)[colnames(data) == "milage"] <- "mileage"
data$mileage <- as.numeric(gsub("[,]| mi\\.", "", data$mileage))
data$accident <- ifelse(data$accident == "At least 1 accident or damage reported", 1, 0)
data$clean_title <-ifelse(data$clean_title == "Yes", 1, 0)
data$Age <- 2024 - data$model_year + 1
data$model_year = NULL
data <- data %>%
  relocate(price, .before = 1)
data$brand = factor(data$brand)

dummy_model <- dummyVars(~ ., data = data)
data_one_hot <- as.data.frame(predict(dummy_model, newdata = data))

sapply(data_one_hot, class)
```
lasso model 
```{r, echo=FALSE}
X = as.matrix(data_one_hot[, -1])
y = as.vector(data_one_hot$price)
lassomodel = cv.glmnet(x = X, y = y, nfolds = 10, alpha = 1)
```

### Original Price Estimation and Comparison  
We selected one car each from Toyota, Ford, and Porsche and using the trained model, predicted the "brand-new" price for each car. Then, we searched online for the actual original release prices of these cars and compared them with the model's predictions. R Code is hidden to make the report more easy to read.  
```{r, echo=FALSE}
set.seed(432)
entiredata <- read.csv("used_cars.csv")
toyota_car <- entiredata[entiredata$brand == "Toyota", ]
toyota_car <- toyota_car[sample(nrow(toyota_car), 1), ]

toyota_new <- data.frame(
  brand.Audi = 0,
  brand.BMW = 0,
  brand.Chevrolet = 0,
  brand.Ford = 0,
  brand.Mercedes.Benz = 0,
  brand.Porsche = 0,
  brand.Toyota = 1,
  mileage = 0,
  accident = 0,
  clean_title = 1,
  Age = 0
)

pred_toyota_new <- predict(lassomodel, newx = as.matrix(toyota_new), s = "lambda.min")
cat("Randomly picked Toyota Car: 4Runner TRD Off Road. Year of Model: 2019\nPredicted Price: 66952.068")
```
The original MSRP was approximately 40,395. Our model predicted the car to be around 26,000 higher than the actual price. 


```{r, echo = FALSE}
set.seed(432)
ford_car <- entiredata[entiredata$brand == "Ford", ]
ford_car <- ford_car[sample(nrow(ford_car), 1), ]
#ford_car
ford_new <- data.frame(
  brand.Audi = 0,
  brand.BMW = 0,
  brand.Chevrolet = 0,
  brand.Ford = 1,
  brand.Mercedes.Benz = 0,
  brand.Porsche = 0,
  brand.Toyota = 0,
  mileage = 0,
  accident = 0,
  clean_title = 1,
  Age = 0
)
pred_ford_new <- predict(lassomodel, newx = as.matrix(ford_new), s = "lambda.min")
#pred_ford_new
cat("Randomly picked Ford Car: Ford Maverick XLT. Year of Model: 2022\nPredicted Price: 67774.82")
```
The original MSRP was approximately 29,541. Our model predicted the car to be around 37,000 higher than the actual price. 


```{r, echo = FALSE}
set.seed(432)
porsche_car <- entiredata[entiredata$brand == "Porsche", ]
porsche_car <- porsche_car[sample(nrow(porsche_car), 1), ]

porsche_new <- data.frame(
  brand.Audi = 0,
  brand.BMW = 0,
  brand.Chevrolet = 0,
  brand.Ford = 0,
  brand.Mercedes.Benz = 0,
  brand.Porsche = 1,
  brand.Toyota = 0,
  mileage = 0,
  accident = 0,
  clean_title = 1,
  Age = 0
)
  
pred_porsche_new <- predict(lassomodel, newx = as.matrix(porsche_new), s = "lambda.min")
#pred_porsche_new
cat("Randomly picked Porsche Car: 718 Spyder Base. Year of Model: 2022\nPredicted Price: 111831.508")
```
The original MSRP was approximately 98300. Our model predicted the car to be around 13500 higher than the actual price. 
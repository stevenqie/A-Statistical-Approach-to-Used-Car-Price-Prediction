---
title: "A Statistical Approach to Used Car Price Prediction"
output:
  pdf_document: default
  word_document: default
date: "12/09/2024"
header-includes:
- \usepackage{multicol}
- \usepackage{fancyvrb}
- \setlength{\columnsep}{1cm}
- \usepackage{geometry}
- \geometry{margin=1in}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\begin{multicols}{3}
\setlength{\columnsep}{2pt}

\begin{center}
\textbf{Steven Qie} \\
\textit{Statistics and Computer Science} \\
\textit{University of Illinois Urbana-Champaign} \\
\textbf{qie2@illinois.edu}
\end{center}

\columnbreak

\begin{center}
\textbf{Brian Gong} \\
\textit{Statistics and Computer Science} \\
\textit{University of Illinois Urbana-Champaign} 
\textbf{brianhg2@illinois.edu}
\end{center}

\columnbreak

\begin{center}
\textbf{William Yeh} \\
\textit{Statistics and Computer Science} \\
\textit{University of Illinois Urbana-Champaign} 
\textbf{wy16@illinois.edu}
\end{center}

\end{multicols}

## Introduction

With the used car market being significantly larger than the new car market, many consumers are realizing that used cars provide a more affordable option. It plays a significant role in the growth and stability of the U.S. economy, driven by changing consumer preferences, economic factors, and the availability of certain cars. Accurately predicting the price of a used car is a challenging but essential task for buyers, sellers, and market analysts/economists alike. 

This report aims to develop various predictive models for used car prices using the Used Car Price Prediction Dataset from Kaggle. This dataset comprises of 4,009 data points, representing unique vehicle listings, as well as nine distinct features that serve as key indicators influencing the value of a used car. We follow a very structured and standard approach, including data exploration, preprocessing, model training, and evaluation using relevant performance metrics. By leveraging these methods, we aim to uncover valuable insights into the world of automobiles and the various factors that are driving used car prices.

Need a section on key findings....



Abstractâ€” 

white space 

white space 

white space 

white space 

white space 

white space 

white space 

white space 

white space 

We utilized AI tools in this report to enhance and assist in our writing. These tools helped play a big role in ensuring clarity, conciseness, and professionalism. We also utilized AI tools to help us with syntax help when writing code in R, as well as discovering potential bugs in our code. 


## Literature Review 
This literature review aims to summarize key findings and approaches from a few noteworthy research papers focused on used car price prediction. 

"Price Prediction of Used Cars Using Machine Learning", written by Chuyang Jin of the University of Sydney, presents a model that can predict a used vehicle's price given their year of production, mileage, tax, miles per gallon, He hopes that his model can benefit and save time for both sellers and buyers who are looking to sell or serach for second-hand vehicles. Jin used a CSV dataset containing 100,000 records of used cars in the UK, focusing specifically on the Mercedes brand. The nine factors that he considered were the following: model, year, selling price, transmission, mileage, fuel type, tax, miles per gallon (mpg), and engine size. While doing exploratory data analysis and preprocessing, Jin noted that many many predictors had skewed distributions. For example, the overwhelming majority of prices fell in the 0-75,000 range, limiting the model's potential effectiveness for higher price ranges. Jin deemed these data points as outliers and excluded them to ensure that the model would be more accurate and usable. After testing various forms of regression, namely linear, polynomial, SVR, Decision Trees, and Random Forests, Jin found Random Forest Regression yielded the best R squared value of 0.90416. 

"Used Car Price Prediction using Machine Learning: A Case Study", written by Mustapha Hankar, Marouane Birjali, and Abderrahim Beni-Hssane, applies several supervised machine learning algorithms to predict used car price prices based on features from a dataset collected from an online eCommerce website called Avito. During preprocessing, the authors of this paper performed recursive feature elimination to maintain only the most relevant features to car prices: year of manufacture, mileage, mark, fuel type, fiscal power, and model. Along with a baseline multiple linear regression model, the study also looked at K-nearest neighbors, Random Forest, Gradient Boosting, and Artificial Neural Networks. The study utilized 2 different performance metrics, R^2 and RMSE, and concluded that the Gradient Boosting Regression Model achieved the best results, with a R^2 of 0.8 and RMSE of 44516.20. 

"Car Price Prediction using Supervised and Unsupervised Learning Models and Deep Learning" by Thomas Nsiah approached the problem of car price prediction from a supervised and unsupervised lenses. While supervised models allow a consumer to understand the key factors and predictors that influence pricing of used cars, unsupervised learning oftentimes uncovers hidden connections and patterns within the data. In his paper, Nsiah used a mock dataset of 50,000 UK second hand car sales with features similar to the previous 2 studies, such as model, engine size, fuel type, year, and mileage. Supervised learning models that Nsiah tried included simple linear regression, polynomial regression, and random forest, evaluated using mean absolute error (MAE) and R-squared metrics. He concluded that out of the supervised models, random forest performed best with an R-squared of 0.99849 and a MAE of 289.0691. For unsupervised learning techniques, Nsiah applied K-Means and DBSCAN clustering to identify price patterns, evaluated using the Davis Boudlin Index and the Silhouette Coefficient. He concluded that K-Means clustering for the year of manufacture vs price produced the best clustering results.

Overall, these three studies demonstrate the effectiveness that machinie learning can have on accurately predicting used car prices. The next section will outline our own approach and findings.

Citations: 

- C. Jin, "Price Prediction of Used Cars Using Machine Learning," in 2021 IEEE International Conference on Emergency Science and Information Technology (ICESIT), Chongqing, China, 2021, pp. 223-230, doi: 10.1109/ICESIT53460.2021.9696839.
- M. Hankar, M. Birjali, and A. Beni-Hssane, "Used Car Price Prediction using Machine Learning: A Case Study," in 2022 11th International Symposium on Signal, Image, Video and Communications (ISIVC), El Jadida, Morocco, 2022, pp. 1-4, doi: 10.1109/ISIVC54825.2022.9800719.
- T. Nsiah, "Car Price Prediction using Supervised and Unsupervised Learning Models and Deep Learning," unpublished, 2024.


## Data Processing and Summary Statistics 

First, we will import the dataset and libraries into our workspace 
```{r, include=FALSE}
library(caret)
library(ggplot2)
library(MASS)
library(randomForest)
library(kernlab)
library(stringr)
library(cluster)
library(glmnet)
library(stats)
library(dplyr)
library(class)
# Load necessary libraries

data <- read.csv("used_cars.csv")
```

#Preliminary Data Cleaning/Modifications
First, we will removed the dollar sign and comma in price to enable numeric operations
```{r, echo=FALSE}
data$price <- as.numeric(gsub("[$,]", "", data$price))
```

Corrected the spelling of mileage from milage to mileage. Removed mi. and , to enable numeric operations. 
```{r, echo=FALSE}
colnames(data)[colnames(data) == "milage"] <- "mileage"
data$mileage <- as.numeric(gsub("[,]| mi\\.", "", data$mileage))
```

The Engine columns contains very useful information such as the horsepower, displacement, cylinders, engine type, and fuel type. We turn these all into new columns. 
```{r, echo=FALSE}
# Extract Horsepower (HP)
data$horsepower <- as.numeric(str_extract(data$engine, "\\d+\\.\\d+(?=HP)"))

# Extract Displacement
data$displacement <- as.numeric(str_extract(data$engine, "\\d+\\.\\d+(?=L)"))

# Extract Cylinders
data$cylinders <- str_extract(data$engine, "\\d+ Cylinder")
#data$cylinders_factor <- factor(str_extract(data$cylinders, "\\d+"))

# Extract Engine Type
data$engine_type <- str_extract(data$engine, "DOHC|SOHC|Turbo|Twin Turbo|Electric Motor")

# Extract Fuel Type
data$fuel_type <- str_extract(data$engine, "Gasoline|Diesel|Electric|Hybrid|Flex Fuel|Plug-In Electric/Gas")
#data$fuel_type_factor <- factor(data$fuel_type)

#we are done with engine column since we have extracted all the information out 
data$engine = NULL
head(data)
```
Looking at each column's type and unique count 
```{r, echo=FALSE}
sapply(data, class)
sapply(data, function(col) {
  if (is.character(col)) {
    length(unique(col))
  } else {
    NA  # Return NA for non-character columns
  }
})
```

Let's examine columns that include NA or Empty String entries. 
```{r, echo=FALSE}
na_columns <- colSums(is.na(data)) > 0
empty_string_columns <- colSums(data == "") > 0
columns_with_na_or_empty <- na_columns | empty_string_columns
print(names(data)[columns_with_na_or_empty])
```

#Analyzing categorical variables
Categorical variables with various unique values include brand, model, transmission, ext_col, int_col. Let's examine all of them

First, we look at the "brand" and the "model" columns. Through analysis shown below, we have decided to omit both of these columns. Our reasoning and visualizations are shown below. 

There are 57 unique brands with the frequency histogram not showing much dominance in a certain brand. To reduce the dimensionality, we will just omit this column 
```{r, echo=FALSE}
length(unique(data$brand)) 
# calculate the counts for brand
brandcounts <- table(data$brand)
barplot(brandcounts,
  main = "Histogram of brand",
  xlab = "brand",
  ylab = "Count",
  col = "skyblue",
  las = 2) 

result <- data %>%
  group_by(brand) %>%
  summarise(
    medianprice = median(price),
    averageprice = mean(price),
    count = n()
  )
result_sorted <- result %>%
  arrange(desc(count))
print(result_sorted)
#omit this column 
data$brand = NULL
```
This problem is seen even more in the model column. We also omit this column from the dataset 
```{r, echo=FALSE}
length(unique(data$model))
modelcounts <- table(data$model)
barplot(modelcounts,
  main = "Histogram of model",
  xlab = "model",
  ylab = "Count",
  col = "skyblue",
  las = 2) 

#omit this columm 
data$model = NULL
```

Now, let's examine colors. There are both intcol and extcol variables. Having too many unique color names can introduce noise into your classification model and make it harder for the model to generalize effectively. Grouping the colors into broader, more general categories can help improve model performance by reducing the dimensionality of the feature and making patterns more apparent.
```{r, echo=FALSE}
# Define the mapping function
generalize_colors <- function(color_name) {
  # Convert to lowercase for uniformity
  color_lower <- tolower(color_name)
  
  # Define patterns for each general category
  if (str_detect(color_lower, "black")) {
    return("Black")
  } else if (str_detect(color_lower, "white|ivory|platinum")) {
    return("White")
  } else if (str_detect(color_lower, "gray|grey|silver|slate|charcoal|mica|metallic|graphite")) {
    return("Gray")
  } else if (str_detect(color_lower, "brown|beige|tan|camel|mocha|walnut|chestnut|saddle|cappuccino|cocoa")) {
    return("Brown")
  } else if (str_detect(color_lower, "silver")) {
    return("Silver")
  } else if (str_detect(color_lower, "gold")) {
    return("Gold")
  } else {
    return("Other")  # For colors that don't match any category
  }
}
```

```{r, echo=FALSE}
length(unique(data$ext_col))
extcolorcounts <- table(data$ext_col)
barplot(extcolorcounts,
  main = "Histogram of ext_col",
  xlab = "model",
  ylab = "Count",
  col = "skyblue",
  las = 2) 
```
Let's apply the generalization function to simply the different colors
```{r, echo=FALSE}
data$ext_col <- sapply(data$ext_col, generalize_colors)
unique(data$ext_col)
```


The same thing happens to int_col, but looking at the dataset we will have 4 categories. 
```{r, echo=FALSE}
length(unique(data$int_col))
intcolorcounts <- table(data$int_col)
barplot(intcolorcounts,
  main = "Histogram of interior color",
  xlab = "model",
  ylab = "Count",
  col = "skyblue",
  las = 2) 

# Grouping less frequent categories
data$int_col <- ifelse(
  data$int_col %in% c("Black", "Jet Black", "AMG Black"),
  "Black",
  ifelse(data$int_col %in% c("Beige", "Ivory"), "Beige/Ivory",
         ifelse(data$int_col %in% c("Gray", "Graphite"), "Gray",
                "Other"))
)

unique(data$int_col)
```

Examining the transmission column now 
```{r, echo=FALSE}
length(unique(data$transmission))
# calculate the counts for transmission
trancounts <- table(data$transmission)

barplot(trancounts,
  main = "Histogram of transmission",
  xlab = "transmission",
  ylab = "Count",
  col = "skyblue",
  las = 2) 

result <- data %>%
  group_by(transmission) %>%
  summarise(
    medianprice = median(price),
    averageprice = mean(price),
    count = n()
  )
result_sorted <- result %>%
  arrange(desc(count))
print(result_sorted)
threshold <- quantile(result_sorted$count, 0.9) # 205.7
significant_transmissions <- result_sorted$transmission[result_sorted$count > threshold]
print(significant_transmissions)
```

Now, we have looked at all the categorical variables with many many unique values, we will now one-hot encode the cagtegorical vairables. 
# One-hot encoding categorical variables 

After looking at histograms for both Brand and Transmission, it seems Brand is more uniformly distributed while Transmission has a few salient categories. After exploring the categories of transmissions we found that the top 7 most frequent transmissions account for approximately 67-70% of the data points. Therefore we will one hot encode these 7 categories + an "Other" category for Transmission for a total of 8 transmission categories. We will also one hot encode "fuel type" and "cylinders" since those are categorical variables as well.

## Transmission

```{r, echo=FALSE}
# map transmissions to just the top 7 or Other. could make this function take in significant_transmissions too for extensibility
map_transmission <- function(transmission) {
  primary_transmissions <- c(
    "A/T", 
    "8-Speed A/T", 
    "Transmission w/Dual Shift Mode", 
    "6-Speed A/T", 
    "6-Speed M/T", 
    "Automatic", 
    "7-Speed A/T"
  )
  
  if (transmission %in% primary_transmissions) {
    return(transmission)
  } else {
    return("Other")
  }
}

# Apply the mapping function
data$transmission <- sapply(data$transmission, map_transmission)
```

#Analyzing Null/Empty Values 
We will first look at the problem with NA and Empty values, something that this dataset has a lot of. We will first handle both NA and Empty "" values by replacing them to "NA" to make it easier to preprocess and analyze.  
```{r, echo=FALSE}
na_columns <- colSums(is.na(data)) > 0
empty_string_columns <- colSums(data == "") > 0
columns_with_na_or_empty <- na_columns | empty_string_columns
print(names(data)[columns_with_na_or_empty])

# data[data == "" | is.na(data)] <- "NA"
# summary(data)
# unique(data$horsepower)
# unique(data$displacement)
# unique(data$cylinders_numeric)
unique(data$fuel_type_factor)
unique(data$fuel_type_numeric)
unique(data$horsepower)
unique(data$displacement)
unique(data$cylinders_factor)
unique(data$cylinders_numeric)
sum(is.na(data$horsepower))
sum(is.na(data$displacement))
table(data$displacement)
summary(data)
```
There are five columns with empty strings/NA values. Let's examine all five of them to discover if we can find any patterns. 

## horsepower 

```{r, echo=FALSE}
# number of unique values in horsepower
length(table(data$horsepower))

# number of null values in horsepower
sum(is.na(data$horsepower))

# calculate the counts for horsepower
horsepower_counts <- table(data$horsepower)

barplot(horsepower_counts,
  main = "Histogram of Horsepower",
  xlab = "Horsepower",
  ylab = "Count",
  col = "skyblue",
  las = 2) 

# median imputation 
data$horsepower[is.na(data$horsepower)] <- median(data$horsepower, na.rm = TRUE)

sum(is.na(data$horsepower))
```

Since there are 348 unique values in horsepower, we can consider horsepower as a continuous variable rather than categorical. However, there are 810 null values in a dataset with 4009 entries which is over 20% null values. This is too many to simply drop, so we want to perform some form of imputation. Looking at the distribution of horsepowers, we can see that the median is a good representative approximation for the distribution so we will use **median imputation**.

## displacement (engine size)

```{r, echo=FALSE}
# number of unique values in displacement
# table(data$displacement)
length(table(data$displacement))

# number of null values in displacement
sum(is.na(data$displacement))

# calculate the counts for horsepower
displacement_counts <- table(data$displacement)

barplot(displacement_counts,
  main = "Histogram of Displacement",
  xlab = "Displacement",
  ylab = "Count",
  col = "skyblue",
  las = 2) 

# median imputation 
data$displacement[is.na(data$displacement)] <- median(data$displacement, na.rm = TRUE)

sum(is.na(data$displacement))
```

There are 61 unique values in displacement (engine size). Although these appear to be discretized measurements (ex: size = 0.8 or size = 3.71 may not make sense), we can treat it as a more continuous predictor for now. There are 396 null values in displacement which is just under 10% null values, so we could consider dropping these. However since the median already exists in the dataset (median = 3.5) we can also proceed with median imputation which is what we did. 

```{r, echo=FALSE}
result <- data %>%
  group_by(fuel_type) %>%
  summarise(
    medianprice = median(price),
    averageprice = mean(price),
    count = n()
  )
print(result)
data$fuel_type[is.na(data$fuel_type)] <- "NA"

```
The NA values for fuel_type have a higher median price and average price than other types, and makes up a significant count of observations so we are going to treat it as a separate category.

#cylinder 
```{r, echo=FALSE}
result <- data %>%
  group_by(cylinders) %>%
  summarise(
    medianprice = median(price),
    averageprice = mean(price),
    count = n()
  )
print(result)
data$cylinders[is.na(data$cylinders)] <- "NA"
```

#accident 
```{r, echo=FALSE}
result <- data %>%
  group_by(accident) %>%
  summarise(
    medianprice = median(price),
    averageprice = mean(price),
    count = n()
  )
print(result)
```
The NA/Empty values for accident exhibit very similar properties to the None reported category, with median price and average price being pretty similar, not to mention a very small percentage of data is represented by this value. Therefore, we replace and combine these observations with the None reported category. Because accident only has 2 unique values now, no accidents and 1 or more accidents, we changed it to 1,0 to be useful for models. 
```{r, echo=FALSE}
data$accident[data$accident == "NA"] <- "None reported"
#unique(data$accident)
data$accident <- ifelse(data$accident == "At least 1 accident or damage reported", 1, 0)
# unique(data$accident)
```

```{r, echo=FALSE}
result <- data %>%
  group_by(clean_title) %>%
  summarise(
    medianprice = median(price),
    averageprice = mean(price),
    count = n()
  )
print(result)
```

The NA values for clean_title clearly have a significantly higher median price and will be treated as a separate category. We apply similar reasoning from accident to clean_title. Since there is only "Yes" and NA, we treat all the yes's to 1 and all the NA values to 0.
```{r, echo=FALSE}
data$clean_title <-ifelse(data$clean_title == "Yes", 1, 0)
unique(data$clean_title)
```

#engine type 
```{r, echo=FALSE}
# calculate the counts for horsepower
engine_counts <- table(data$engine_type)

barplot(engine_counts,
  main = "Histogram of engine_type",
  xlab = "Engine Type",
  ylab = "Count",
  col = "skyblue",
  las = 2) 

result <- data %>%
  group_by(engine_type) %>%
  summarise(
    medianprice = median(price),
    averageprice = mean(price),
    count = n()
  )
print(result)

data$engine_type[is.na(data$engine_type)] <- "NA"
```

#Removing Outliers 
We remove outliers with 1.5*IQR value.
```{r, echo=FALSE}
Q1 <- quantile(data$price, 0.25, na.rm = TRUE)
Q3 <- quantile(data$price, 0.75, na.rm = TRUE)
IQR_value <- IQR(data$price, na.rm = TRUE)

# Identify outliers using IQR
lower_bound <- Q1 - 1.5 * IQR_value
upper_bound <- Q3 + 1.5 * IQR_value

outliers <- data[data$price < lower_bound | data$price > upper_bound, ]
print(paste("Number of outliers: ", nrow(outliers), "and average price of these cars: ", round(mean(outliers$price), 2)))
      
#removing these rows from the dataset
data <- data[!(data$price < lower_bound | data$price > upper_bound), ]
```

```{r, echo=FALSE}
summary(data)
```
#turning each categorial column into a factor type 
```{r, echo=FALSE}
data[sapply(data, is.character)] <- lapply(data[sapply(data, is.character)], as.factor)
```


#one hot encoding
Some models will require one hot encoding. For these models, we create a new dataset and apply this one hot encoding 
```{r, echo=FALSE}
dummy_model <- dummyVars(~ ., data = data)
data_one_hot <- as.data.frame(predict(dummy_model, newdata = data))
```


#Final Summary Statistics
```{r, echo=FALSE}
dim(data_one_hot)
summary(data_one_hot)
```

## Unsupervised Learning 
Apply at least three clustering algorithms to the processed dataset.
Determine the appropriate number of clusters and discuss the interpretability of these clusters. Do they hold any meaningful distinctions?
Examine whether the clustering results are associated with your outcome variable.

1. KMeans Clustering 
```{r, echo=FALSE}
data_subset <- data[, c("model_year", "price")]
data_subset <- na.omit(data_subset)
data_subset_scaled <- scale(data_subset)
```
We decided to use kmeans to examine the relation between model_year and price, as we noticed a similar examination in one of the papers while doing the literature review. Because K-means utilizes distance metrics, we scale the data before clustering.

```{r, echo=FALSE}
set.seed(1)

sil_scores <- sapply(2:10, function(k) {
  km <- kmeans(scale(data_subset_scaled[, c("model_year", "price")]), centers = k, nstart = 10)
  silhouette(km$cluster, dist(scale(data_subset[, c("model_year", "price")]))) %>%
    summary() %>%
    .$avg.width
})

plot(2:10, sil_scores, type = "b", pch = 19, frame = FALSE,
     xlab = "Number of Clusters (k)",
     ylab = "Average Silhouette Score",
     main = "Silhouette Method for Optimal k")

```
We decided to use the Silhouette Method to determine the optimal number of clusters. This method essentially uses distance measures calculating how close clusters are to themselves and how far away they are to other clusters to judge the optimal number of clusters. In this case, 2 has the highest average silhouette score so we will use k=2.


```{r, echo=FALSE}
set.seed(1)  

kmeans_result <- kmeans(data_subset_scaled, centers = 2)
data_subset$cluster <- as.factor(kmeans_result$cluster)

ggplot(data_subset, aes(x = model_year, y = price, color = cluster)) +
  geom_point(size = 2) +
  labs(title = "K-Means Clustering on Model Year vs Price",
       x = "Model Year",
       y = "Price") +
  theme_minimal()

```
There seems to be a pretty solid relationship between a more recent model_year and higher price. Although the 2 clusters seem to be mostly dominated by model year, it's clear that the average price of cluster 2 is higher than cluster 1.



2. Hierarchical Clustering 

Next, we will try hierarchical clustering with three different linkage methods(single, complete, and average) using euclidean distance. Hierarchical Clustering begins with each data point starting as its own cluster. The goal is to progressively group them together until there is only one group. The process involves choosing the closest two groups, calculated through a specific distance metric. 
```{r, echo=FALSE}
#numeric_data <- data[, c("model_year", "price")]
numeric_data <- data[, sapply(data, is.numeric)]
numeric_data_without_price <- numeric_data[, !colnames(numeric_data) %in% "price"]

```
Removing non-numeric features as clustering requires numeric features. Also, removed the target feature price.

```{r, echo=FALSE}

# Perform hierarchical clustering with scaled data
hclust_single <- hclust(dist(numeric_data_without_price, method = "euclidean"), method = "single")
hclust_complete <- hclust(dist(numeric_data_without_price, method = "euclidean"), method = "complete")
hclust_average <- hclust(dist(numeric_data_without_price, method = "euclidean"), method = "average")

# Plot the dendrograms
par(mfrow = c(1, 3))  # Arrange plots side by side
plot(hclust_single, main = "Single Linkage", xlab = "", sub = "", cex = 0.6)
plot(hclust_complete, main = "Complete Linkage", xlab = "", sub = "", cex = 0.6)
plot(hclust_average, main = "Average Linkage", xlab = "", sub = "", cex = 0.6)
par(mfrow = c(1, 1))  # Reset plotting layout


```



```{r, echo=FALSE}
# Cut the dendrogram into 2 clusters using complete linkage
clusters_complete <- cutree(hclust_complete, k = 2)

# Assign cluster labels to the dataset
data$cluster <- as.factor(clusters_complete)

# View the cluster sizes
table(data$cluster)

# Summarise cluster statistics
library(dplyr)

cluster_summary <- data %>%
  group_by(cluster) %>%
  summarise(
    avg_price = mean(price, na.rm = TRUE),
    avg_model_year = mean(model_year, na.rm = TRUE),
    avg_accident = mean(as.numeric(accident), na.rm = TRUE),  # Convert accident to numeric if necessary
    avg_mileage = mean(mileage, na.rm = TRUE),
    avg_horsepower = mean(horsepower, na.rm = TRUE),
    count = n()
  )

# Print the cluster summary
print(cluster_summary)




```
There are a lot of correlations here that make sense between the 2 clusters. Cluster 1, with a more recent avg_model_year, also has a lower avg_mileage and a lower avg_accident rate, probably because the car has been driven for less time, this cluster also has a much higher avg_price in comparison to cluster 2. The data isn't distributed very well however as a vast majority of the points sit in cluster 1, perhaps suggesting that hierarchical clustering isn't suitable for this dataset.


3. Spectral Clustering 

Finally, we will try spectral clustering, which aims to group observations based on their proximity information. This method involves 2 main steps, the first being using the eigenvalues of a similarity matrix to perform dimension reduction, followed by applying a clustering algorithm like K-means. 

```{r, echo=FALSE}

# Step 1: Prepare and scale data
# Select numeric columns only
numeric_data <- data[, sapply(data, is.numeric)]  
numeric_data <- numeric_data[sample(nrow(numeric_data)), ]
numeric_data_without_price <- numeric_data[, !colnames(numeric_data) %in% "price"]  # Exclude 'price' column
numeric_data_without_price_scaled <- scale(numeric_data_without_price)  # Scale the data

# Step 2: Subset the first 1000 points
subset_data <- numeric_data_without_price_scaled[1:1000, ]

# Step 3: Perform spectral clustering
set.seed(1)  # For reproducibility
n_clusters <- 2  # Number of clusters
specc_result <- specc(as.matrix(subset_data), centers = n_clusters, kernel = "rbfdot")

# Step 4: Add cluster assignments to the original dataset
data$cluster <- NA  # Initialize cluster column
data$cluster[1:1000] <- as.factor(specc_result@.Data)  # Assign clusters to the first 1000 points

# Step 5: Summarize the clusters
cluster_summary <- data %>%
  filter(!is.na(cluster)) %>%
  group_by(cluster) %>%
  summarise(
    avg_model_year = mean(model_year, na.rm = TRUE),
    avg_mileage = mean(mileage, na.rm = TRUE),
    avg_accident = mean(as.numeric(accident), na.rm = TRUE),  # Convert 'accident' to numeric if necessary
    avg_horsepower = mean(horsepower, na.rm = TRUE),
    count = n()
  )

# Print the cluster summary
print(cluster_summary)

# Step 6: Visualize the clusters (optional)
ggplot(data %>% filter(!is.na(cluster)), aes(x = model_year, y = mileage, color = cluster)) +
  geom_point(size = 2) +
  labs(title = "Spectral Clustering Results (First 1000 Points)", x = "Model Year", y = "Mileage") +
  theme_minimal()




```
Similar to Cluster 1, with a more recent avg_model_year, also has a lower avg_mileage and a lower avg_accident rate, this cluster also has a much higher avg_price in comparison to cluster 2. The distribution of data points between the 2 clusters seem to be more even in comparison to heirarchicaly clustering, meaning that perhaps spectral clustering is more suitable for this dataset.


## Prediction Models 

For all the supervised models below, we will split the data into training sets for model training and testing sets to evaluate performance and accuracy
```{r, echo=FALSE}
y = data_one_hot$price
X <- data_one_hot[, !(colnames(data_one_hot) %in% "price")]

sample <- sample(c(TRUE, FALSE), nrow(data), replace=TRUE, prob=c(0.8, 0.2))
xtrain <- X[sample, ]
xtest <- X[!sample, ]
ytrain = y[sample]
ytest = y[!sample] 

```

1. Linear Model. There are mainly three possible linear models: Lasso, Ridge, and Elastic Net. We will try all three models and see which one performs the best. Lasso, Ridge, and Elastic Net all benefit from feature scaling because these models involve regularization. which will penalize the size of coefficients of the model to avoid overfitting. All 3 models also involving a tuning parameter, and so we will use k-fold cross validation to find the best parameters. cv.glmnet will automatically scale and center the data as well. 

Training our ridge model 
```{r, echo=FALSE}
ridgemodel = cv.glmnet(x = as.matrix(xtrain), y = ytrain, nfolds = 10, alpha = 0)
```

```{r, echo=FALSE}
ridgemodel$lambda.min

pred = predict(ridgemodel, newx = as.matrix(xtest), s = "lambda.min")
sqrt(mean((pred - ytest)^2))
```

Training our lasso model 
```{r, echo=FALSE}
lassomodel = cv.glmnet(x = as.matrix(xtrain), y = ytrain, nfolds = 10, alpha = 1)
```

```{r, echo=FALSE}
lassomodel$lambda.min

pred = predict(lassomodel, newx = as.matrix(xtest), s = "lambda.min")
sqrt(mean((pred - ytest)^2))
```

Training our elastic net model 
```{r, echo=FALSE}
elastic_net_model <- cv.glmnet(x = as.matrix(xtrain), y = ytrain, nfolds = 10, alpha = 0.5)
```

```{r, echo=FALSE}
elastic_net_model$lambda.min

pred1 = predict(elastic_net_model, newx = as.matrix(xtest), s = "lambda.min")
sqrt(mean((pred1 - ytest)^2))
```

Out of our 3 linear models, Ridge performed the best, with a RMSE of 12261.19 


2. K Nearest Neighbors(KNN) regression works by calculating the k nearest training set data points to the test point and predicting the target value by taking the average of their target values. KNN is sensitive to feature scaling, so we will need to scale the data. The reason behind this is for example, if one feature has ranges from 1-10 and another one has 1-10000, distance calcualtions will be biased and results will suffer as a result. KNN is also sensitive to the choice of k. To find the optimal value of k, we will perform k-fold cross validation. 
```{r, echo=FALSE}
set.seed(1)

# 2. Scale the numeric columns
preProcValues <- preProcess(xtrain, method = c("center", "scale"))

# Apply scaling and centering to the training and test data
xtrain_processed <- predict(preProcValues, xtrain)
xtest_processed <- predict(preProcValues, xtest)

pca_model <- prcomp(xtrain_processed, center = TRUE, scale. = TRUE)
explained_variance <- summary(pca_model)$importance[3, ]  # Cumulative variance
num_components <- which(explained_variance >= 0.95)[1]   # First component to reach 95%

# Print the number of components
cat("Number of components to retain:", num_components, "\n")

# Transform the training and testing data
xtrain_pca <- pca_model$x[, 1:num_components]  # Retain the first num_components
xtest_pca <- as.matrix(xtest_processed) %*% pca_model$rotation[, 1:num_components]

# 3. Train the KNN model using the processed data
control <- trainControl(method = "cv", number = 10)
knn.cvfit <- train(ytrain ~ ., method = "knn", 
                   data = data.frame(xtrain_pca, ytrain),
                   tuneGrid = data.frame(k = seq(1, 45, 1)),
                   trControl = control)

# 4. Plot the cross-validation results
plot(knn.cvfit)

# 6. Print the best value of k based on cross-validation
print(paste("The best value of k based on cross-validation is: ", knn.cvfit$bestTune$k))
```

```{r, echo=FALSE}
# Train the final model using the best value of k and find the predictions
best_k <- knn.cvfit$bestTune$k
knn_predictions <- knn(train = xtrain_processed, test = xtest_processed, cl = ytrain, k = best_k)

# Calculate prediction error 
print(paste("Prediction errort: ", sqrt(mean((as.numeric(knn_predictions) - ytest)^2))))
```


3. Random Forest 
4. SVM? does this count as a linear model 
5. Gradient Boosting Regressor 

## Open-Ended Question/Conclusion
A researcher has reached out and is interested in estimating the original price of the cars in our dataset as if they were brand new. 

To solve this problem, we will follow a similar approach by building a machine learning model using the most features that are typically related to depreciation. Additionally, modeling depreciation is usually something that is not linear. A car brand's value might lose a large portion of it's value in the first year and then depreciates more slowly afterward, while another model might hold it's value better in the long run. Also, for example, if a car get's in an accident, suddenly it's price will plummet. Therefore, we will try non-linear models such as random forest and use the following features that we deemed to be especially important for understanding depreciation. 

1. Brand. Some brands might hold their value better than others. Economy brands like Ford and Toyota may display a more linear depreciation, while luxary brands like BMW and Mercedes might see a more steep initial depreciation. Whether or not a brand is a luxary or economy is pretty benefifical, as luxary brands will typically have higher starting prices than economy brands. This is why we decide to include this for calcalitng new prices and decided to omit it in our prediction models above. 
2. Age. Age is one of the most fundamental factors in depreciation. Generally, cars will lose value over time since newer models with updated features get released. In out dataset, since we are only given the model_year of when the car was manufactured, we create a new column called "Age" that is simply current_year - model_year + 1. This gives us the number of years that have passed since the car was new. 
3. Mileage. Mileage is another fundamental factor in depreciation. It is a indicator of how much the car has been used. Generally, higher mileage correlates with lower value, as more maintenance may be required to keep it healthy and running. 
4. Accident History. Accidents will significantly decrease the value of the car, which contributes to depreciation.   
5. Clean Title. A clean title indicates that there has been no legal/insurance issues with the car. Similar to accident history, a car without a clean title may depreciate more quickly because of the higher perceived risk. 

The task of estimating brand new car prices using only a dataset of used car data can be challenging. The biggest limitating factor is that because the model has never actually seen cars at zero age and zero mileage, it has to infer and extrapolate what the car might have cost when it was first bought, leading to potential inaccuracies. There are also a lot of other external factors that the dataset doesn't capture, such as inflation, competition, technological advancements, and special promotions all have the ability to shift pricing strategies and patterns. For example, a certain used car's price might have been during a time of inflation, which may not align with the pricing logic for the car's original release date. 

To avoid model complexity that arises from having to one-hot encoding every single brand and practicality reasons, we will train our model on a subset of the data, mainly used cars that belong to the 7 most common brands in the dataset: Ford, BMW, Mercedes-Benz, Chevrolet, Porsche, Audi, Toyota. This is a good mix of both luxary and economy brands. 
```{r, echo=FALSE}
data <- read.csv("used_cars.csv")
data <- data %>%
  select(brand, model_year, milage, accident, clean_title, price)
data <- data %>% filter(brand %in% c("Ford", "BMW", "Mercedes-Benz", "Chevrolet", "Porsche", "Audi", "Toyota"))

data$price <- as.numeric(gsub("[$,]", "", data$price))
colnames(data)[colnames(data) == "milage"] <- "mileage"
data$mileage <- as.numeric(gsub("[,]| mi\\.", "", data$mileage))
data$accident <- ifelse(data$accident == "At least 1 accident or damage reported", 1, 0)
data$clean_title <-ifelse(data$clean_title == "Yes", 1, 0)
data$Age <- 2024 - data$model_year + 1
data$model_year = NULL
data <- data %>%
  relocate(price, .before = 1)
data$brand = factor(data$brand)

dummy_model <- dummyVars(~ ., data = data)
data_one_hot <- as.data.frame(predict(dummy_model, newdata = data))

sapply(data_one_hot, class)
```

lasso model 
```{r, echo=FALSE}
X = as.matrix(data_one_hot[, -1])
y = as.vector(data_one_hot$price)
lassomodel = cv.glmnet(x = X, y = y, nfolds = 10, alpha = 1)
```

Let's do the task of selecting three cars from your dataset and estimating their price as if they were new. Let's first select a random toyota car 
```{r, echo=FALSE}
set.seed(1)
entiredata <- read.csv("used_cars.csv")
toyota_car <- entiredata[entiredata$brand == "Toyota", ]
toyota_car <- toyota_car[sample(nrow(toyota_car), 1), ]
#print("Randomly picked Toyota Car:", toyota_car$model)
#print("Year of model:", toyota_car$model_year)
toyota_car

toyota_new <- data.frame(
  brand.Audi = 0,
  brand.BMW = 0,
  brand.Chevrolet = 0,
  brand.Ford = 0,
  brand.Mercedes.Benz = 0,
  brand.Porsche = 0,
  brand.Toyota = 1,
  mileage = 0,
  accident = 0,
  clean_title = 1,
  Age = 0
)

pred_toyota_new <- predict(lassomodel, newx = as.matrix(toyota_new))
print(paste("the new price is ", pred_toyota_new))

```
